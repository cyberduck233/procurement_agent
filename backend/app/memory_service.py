"""
è®°å¿†æœåŠ¡ - è´Ÿè´£è®°å¿†çš„æå–ã€æ£€ç´¢ã€å­˜å‚¨å’Œç®¡ç†
"""
from __future__ import annotations

import json
import logging
import re
from difflib import SequenceMatcher
from typing import List, Optional, Dict, Any, Tuple

import httpx
from sqlalchemy.orm import Session

from .config import Settings
from .database import (
    Memory,
    SessionConfig,
    create_memory,
    get_memory_by_id,
    search_memories,
    update_memory,
    update_memory_access,
    get_session_config,
)
from .rag_service import get_embeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma

logger = logging.getLogger(__name__)

# è®°å¿†å‘é‡æ•°æ®åº“ç¼“å­˜
_MEMORY_VECTORSTORE_CACHE: Dict[str, Chroma] = {}


# ==================== è®°å¿†æå–æ¨¡å— ====================

async def extract_memories_from_conversation(
    conversation_text: str,
    settings: Settings,
    session_id: str,
    user_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ LLM ä»å¯¹è¯ä¸­æå–é‡è¦ä¿¡æ¯ä½œä¸ºè®°å¿†
    
    Args:
        conversation_text: å¯¹è¯æ–‡æœ¬
        settings: é…ç½®å¯¹è±¡
        session_id: ä¼šè¯ID
        user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æå–çš„è®°å¿†åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å¿†åŒ…å« type, content, importance
    """
    try:
        extraction_prompt = f"""åˆ†æä»¥ä¸‹å¯¹è¯ï¼Œæå–åº”è¯¥è¢«é•¿æœŸè®°ä½çš„é‡è¦ä¿¡æ¯ã€‚

å¯¹è¯å†…å®¹ï¼š
{conversation_text}

è¯·æå–ä»¥ä¸‹ç±»å‹çš„ä¿¡æ¯ï¼š
1. **fact** - æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼ˆå¦‚ï¼šç”¨æˆ·çš„åå­—ã€èŒä¸šã€å·¥ä½œåœ°ç‚¹ã€å±…ä½åœ°ã€å¹´é¾„ã€æŠ€èƒ½ç­‰ï¼‰
   - **ç‰¹åˆ«æ³¨æ„**ï¼šå¦‚æœå¯¹è¯ä¸­æåˆ°ç”¨æˆ·çš„åå­—ï¼Œå¿…é¡»æå–ä¸º fact ç±»å‹ï¼Œé‡è¦æ€§è®¾ä¸º 90-100
   - ä¾‹å¦‚ï¼š"æˆ‘å«å¼ ä¸‰" â†’ {{"type": "fact", "content": "ç”¨æˆ·çš„åå­—æ˜¯å¼ ä¸‰", "importance": 95}}
2. **preference** - ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯ï¼ˆå¦‚ï¼šå–œæ¬¢çš„é£Ÿç‰©ã€ç¼–ç¨‹è¯­è¨€ã€å·¥ä½œä¹ æƒ¯ã€å…´è¶£çˆ±å¥½ç­‰ï¼‰
3. **event** - é‡è¦çš„äº‹ä»¶æˆ–è®¡åˆ’ï¼ˆå¦‚ï¼šç”Ÿæ—¥ã€ä¼šè®®å®‰æ’ã€æ—…è¡Œè®¡åˆ’ã€é‡è¦æ—¥æœŸç­‰ï¼‰
4. **relationship** - äººç‰©å…³ç³»æˆ–ç¤¾äº¤ä¿¡æ¯ï¼ˆå¦‚ï¼šå®¶äººã€æœ‹å‹ã€åŒäº‹å…³ç³»ç­‰ï¼‰

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºæå–çš„è®°å¿†ï¼š
{{
  "memories": [
    {{
      "type": "fact|preference|event|relationship",
      "content": "è®°å¿†å†…å®¹çš„ç®€æ´æè¿°ï¼ˆä½¿ç”¨ç¬¬ä¸‰äººç§°ï¼Œå¦‚'ç”¨æˆ·çš„åå­—æ˜¯XXX'ï¼‰",
      "importance": 50-100
    }}
  ]
}}

è¦æ±‚ï¼š
1. **å¿…é¡»æå–ç”¨æˆ·å§“å**ï¼šå¦‚æœå¯¹è¯ä¸­æåˆ°ç”¨æˆ·çš„åå­—ï¼ˆå¦‚"æˆ‘å«XXX"ã€"æˆ‘æ˜¯XXX"ï¼‰ï¼Œå¿…é¡»æå–ä¸º fact ç±»å‹ï¼Œimportance è®¾ä¸º 90-100
2. åªæå–çœŸæ­£é‡è¦ã€å€¼å¾—é•¿æœŸè®°ä½çš„ä¿¡æ¯
3. å†…å®¹è¦ç®€æ´ã€æ˜ç¡®ï¼Œä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°ï¼ˆå¦‚"ç”¨æˆ·çš„åå­—æ˜¯XXX"è€Œä¸æ˜¯"æˆ‘çš„åå­—æ˜¯XXX"ï¼‰
4. importance è¯„åˆ†è¦åˆç†ï¼š
   - å§“åã€èŒä¸šç­‰å…³é”®ä¿¡æ¯ï¼š90-100
   - é‡è¦åå¥½ã€äº‹ä»¶ï¼š70-89
   - ä¸€èˆ¬ä¿¡æ¯ï¼š50-69
5. å¦‚æœå¯¹è¯ä¸­æ²¡æœ‰é‡è¦ä¿¡æ¯ï¼Œè¿”å›ç©ºçš„ memories æ•°ç»„
6. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Š
"""

        headers = {
            "Authorization": f"Bearer {settings.deepseek_api_key}",
            "Content-Type": "application/json",
        }
        endpoint = f"{settings.deepseek_base_url.rstrip('/')}/chat/completions"

        payload = {
            "model": "deepseek-r1",
            "messages": [{"role": "user", "content": extraction_prompt}],
            "temperature": 0.3,
            "max_tokens": 1000,
            "stream": False,
        }

        async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
            response = await client.post(endpoint, json=payload, headers=headers)

        if response.status_code != 200:
            logger.error(f"è®°å¿†æå– API é”™è¯¯ {response.status_code}: {response.text}")
            return []

        data = response.json()
        reply_text = data["choices"][0]["message"]["content"]

        # è§£æ JSON å“åº”
        memories = _parse_memory_extraction(reply_text)

        logger.info(f"ä»å¯¹è¯ä¸­æå–äº† {len(memories)} æ¡è®°å¿†")
        return memories

    except Exception as e:
        logger.error(f"è®°å¿†æå–å¤±è´¥: {e}", exc_info=True)
        return []


def _parse_memory_extraction(text: str) -> List[Dict[str, Any]]:
    """è§£æ LLM è¿”å›çš„è®°å¿†æå–ç»“æœ"""
    try:
        # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        data = json.loads(cleaned)
        memories = data.get("memories", [])

        # éªŒè¯å’Œæ¸…ç†è®°å¿†æ•°æ®
        validated = []
        for mem in memories:
            mem_type = mem.get("type", "").lower()
            if mem_type not in ["fact", "preference", "event", "relationship"]:
                continue
            
            content = mem.get("content", "").strip()
            if not content:
                continue
            
            importance = int(mem.get("importance", 50))
            importance = max(50, min(100, importance))

            validated.append({
                "type": mem_type,
                "content": content,
                "importance": importance,
            })

        return validated

    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {text[:200]}")
        return []
    except Exception as e:
        logger.error(f"è®°å¿†è§£æå¤±è´¥: {e}", exc_info=True)
        return []


# ==================== ç›¸ä¼¼åº¦æ£€æµ‹æ¨¡å— ====================

def calculate_text_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    ä½¿ç”¨ SequenceMatcher è®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
    """
    if not text1 or not text2:
        return 0.0
    
    # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤å¤šä½™ç©ºæ ¼
    text1 = re.sub(r'\s+', ' ', text1.lower().strip())
    text2 = re.sub(r'\s+', ' ', text2.lower().strip())
    
    if text1 == text2:
        return 1.0
    
    similarity = SequenceMatcher(None, text1, text2).ratio()
    return similarity


def calculate_jaccard_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ Jaccard ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
    åŸºäºè¯æ±‡é›†åˆçš„äº¤é›†å’Œå¹¶é›†
    """
    if not text1 or not text2:
        return 0.0
    
    # åˆ†è¯ï¼ˆç®€å•åˆ†è¯ï¼ŒæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    words1 = set(re.findall(r'\w+', text1.lower()))
    words2 = set(re.findall(r'\w+', text2.lower()))
    
    if not words1 or not words2:
        return 0.0
    
    intersection = len(words1 & words2)
    union = len(words1 | words2)
    
    if union == 0:
        return 0.0
    
    return intersection / union


def find_similar_memory(
    session: Session,
    new_content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    threshold: float = 0.75,
) -> Optional[Tuple[Memory, float]]:
    """
    æŸ¥æ‰¾ä¸æ–°è®°å¿†ç›¸ä¼¼çš„å·²æœ‰è®°å¿†
    
    æ³¨æ„ï¼šå»é‡æ£€æŸ¥æ—¶å…¨å±€æœç´¢ï¼ˆå¿½ç•¥ session_idï¼‰ï¼Œé¿å…åœ¨ä¸åŒä¼šè¯ä¸­åˆ›å»ºé‡å¤è®°å¿†
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        new_content: æ–°è®°å¿†çš„å†…å®¹
        memory_type: è®°å¿†ç±»å‹
        user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œä½†åœ¨å»é‡æ—¶ä¼šè¢«å¿½ç•¥ï¼‰
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºç›¸ä¼¼
    
    Returns:
        (ç›¸ä¼¼è®°å¿†, ç›¸ä¼¼åº¦) æˆ– None
    """
    # è·å–åŒç±»å‹çš„è®°å¿†ï¼ˆå»é‡æ—¶å…¨å±€æœç´¢ï¼Œä¸é™åˆ¶ session_idï¼‰
    similar_memories = search_memories(
        session=session,
        memory_type=memory_type,
        user_id=user_id,
        session_id=None,  # ğŸ”§ å»é‡æ—¶å¿½ç•¥ session_idï¼Œå…¨å±€æœç´¢é¿å…é‡å¤
        limit=50,  # å¢åŠ æœç´¢èŒƒå›´ä»¥æé«˜å»é‡å‡†ç¡®ç‡
    )
    
    logger.info(f"ğŸ” å»é‡æ£€æŸ¥ï¼šæ–°è®°å¿†='{new_content[:50]}...'ï¼Œæ‰¾åˆ° {len(similar_memories)} æ¡åŒç±»å‹è®°å¿†")
    
    if not similar_memories:
        logger.info(f"âœ¨ æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼è®°å¿†ï¼Œå°†åˆ›å»ºæ–°è®°å¿†")
        return None
    
    best_match = None
    best_similarity = 0.0
    
    # è®¡ç®—ä¸æ¯æ¡è®°å¿†çš„ç›¸ä¼¼åº¦
    for memory in similar_memories:
        # æ–‡æœ¬ç›¸ä¼¼åº¦
        text_sim = calculate_text_similarity(new_content, memory.content)
        # Jaccard ç›¸ä¼¼åº¦
        jaccard_sim = calculate_jaccard_similarity(new_content, memory.content)
        
        # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆæ–‡æœ¬ç›¸ä¼¼åº¦æƒé‡0.6ï¼ŒJaccardç›¸ä¼¼åº¦æƒé‡0.4ï¼‰
        combined_sim = text_sim * 0.6 + jaccard_sim * 0.4
        
        logger.debug(f"ç›¸ä¼¼åº¦å¯¹æ¯”: æ–°è®°å¿†='{new_content[:50]}...' vs å·²æœ‰='{memory.content[:50]}...' => text_sim={text_sim:.3f}, jaccard_sim={jaccard_sim:.3f}, combined={combined_sim:.3f}")
        
        if combined_sim > best_similarity:
            best_similarity = combined_sim
            best_match = memory
    
    # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›æœ€ä½³åŒ¹é…
    if best_match and best_similarity >= threshold:
        logger.info(f"ğŸ¯ å‘ç°ç›¸ä¼¼è®°å¿†: {best_match.id}, ç›¸ä¼¼åº¦: {best_similarity:.3f} (é˜ˆå€¼={threshold}), å°†åˆå¹¶")
        return (best_match, best_similarity)
    
    if best_match:
        logger.info(f"âš ï¸ æœ€ç›¸ä¼¼è®°å¿†ç›¸ä¼¼åº¦ {best_similarity:.3f} æœªè¾¾åˆ°é˜ˆå€¼ {threshold}ï¼Œå°†åˆ›å»ºæ–°è®°å¿†")
    
    return None


# ==================== è®°å¿†åˆå¹¶æ¨¡å— ====================

def merge_similar_memories(
    session: Session,
    existing_memory: Memory,
    new_content: str,
    new_importance: int,
    new_metadata: Optional[Dict[str, Any]] = None,
) -> Memory:
    """
    å°†æ–°è®°å¿†åˆå¹¶åˆ°å·²æœ‰è®°å¿†ä¸­
    
    ç­–ç•¥ï¼š
    1. å¦‚æœæ–°å†…å®¹æ›´å®Œæ•´æˆ–æ›´è¯¦ç»†ï¼Œæ›´æ–°å†…å®¹
    2. å–æ›´é«˜çš„é‡è¦æ€§è¯„åˆ†
    3. åˆå¹¶å…ƒæ•°æ®
    4. æ›´æ–°è®¿é—®ç»Ÿè®¡
    """
    # åˆ¤æ–­å“ªä¸ªå†…å®¹æ›´å¥½ï¼ˆæ›´é•¿æˆ–åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
    existing_content = existing_memory.content
    should_update_content = False
    
    # å¦‚æœæ–°å†…å®¹æ›´é•¿æˆ–åŒ…å«æ›´å¤šå…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯æ›´å¥½çš„ç‰ˆæœ¬
    if len(new_content) > len(existing_content) * 1.2:
        should_update_content = True
    elif len(new_content) > len(existing_content):
        # æ–°å†…å®¹ç¨é•¿ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æ›´å¤šä¿¡æ¯
        new_words = set(re.findall(r'\w+', new_content.lower()))
        existing_words = set(re.findall(r'\w+', existing_content.lower()))
        if len(new_words - existing_words) > len(existing_words - new_words):
            should_update_content = True
    
    # æ›´æ–°å†…å®¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    final_content = new_content if should_update_content else existing_content
    
    # å–æ›´é«˜çš„é‡è¦æ€§è¯„åˆ†
    final_importance = max(existing_memory.importance_score, new_importance)
    
    # å‡†å¤‡å…ƒæ•°æ®
    merged_metadata = new_metadata or {}
    merged_metadata["merged_count"] = merged_metadata.get("merged_count", 0) + 1
    merged_metadata["similarity_merge"] = True
    
    # æ›´æ–°è®°å¿†
    updated_memory = update_memory(
        session=session,
        memory_id=existing_memory.id,
        content=final_content if should_update_content else None,
        importance_score=final_importance,
        metadata=merged_metadata,
    )
    
    logger.info(
        f"åˆå¹¶è®°å¿†: {existing_memory.id}, "
        f"å†…å®¹{'å·²æ›´æ–°' if should_update_content else 'ä¿ç•™'}, "
        f"é‡è¦æ€§: {final_importance}"
    )
    
    return updated_memory


def save_memory_with_dedup(
    session: Session,
    content: str,
    memory_type: str,
    importance_score: int = 50,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    tags: Optional[list[str]] = None,
    metadata: Optional[dict] = None,
    threshold: float = 0.75,
) -> Memory:
    """
    ä¿å­˜è®°å¿†ï¼Œå¹¶åœ¨ä¿å­˜å‰æ£€æŸ¥é‡å¤å¹¶åˆå¹¶
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        content: è®°å¿†å†…å®¹
        memory_type: è®°å¿†ç±»å‹
        importance_score: é‡è¦æ€§è¯„åˆ†
        user_id: ç”¨æˆ·ID
        session_id: ä¼šè¯ID
        tags: æ ‡ç­¾åˆ—è¡¨
        metadata: å…ƒæ•°æ®
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
    
    Returns:
        ä¿å­˜æˆ–åˆå¹¶åçš„è®°å¿†å¯¹è±¡
    """
    # æŸ¥æ‰¾ç›¸ä¼¼è®°å¿†
    similar_result = find_similar_memory(
        session=session,
        new_content=content,
        memory_type=memory_type,
        user_id=user_id,
        session_id=session_id,
        threshold=threshold,
    )
    
    if similar_result:
        existing_memory, similarity = similar_result
        # åˆå¹¶åˆ°å·²æœ‰è®°å¿†
        merged_metadata = metadata or {}
        merged_metadata["similarity_score"] = similarity
        
        merged_memory = merge_similar_memories(
            session=session,
            existing_memory=existing_memory,
            new_content=content,
            new_importance=importance_score,
            new_metadata=merged_metadata,
        )
        
        return merged_memory
    else:
        # æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼è®°å¿†ï¼Œä¿å­˜æ–°è®°å¿†
        new_memory = create_memory(
            session=session,
            content=content,
            memory_type=memory_type,
            importance_score=importance_score,
            user_id=user_id,
            session_id=session_id,
            tags=tags,
            metadata=metadata,
        )
        
        logger.info(f"åˆ›å»ºæ–°è®°å¿†: {new_memory.id}, ç±»å‹: {memory_type}")
        return new_memory


# ==================== å‘é‡å­˜å‚¨æ¨¡å— ====================

def get_memory_vectorstore(settings: Settings) -> Chroma:
    """
    è·å–è®°å¿†å‘é‡æ•°æ®åº“ï¼ˆç‹¬ç«‹çš„ collectionï¼‰
    ä½¿ç”¨ç‹¬ç«‹çš„ collection å­˜å‚¨è®°å¿†ï¼Œä¸æ–‡æ¡£åˆ†å¼€
    """
    key = str(settings.chroma_dir)
    store = _MEMORY_VECTORSTORE_CACHE.get(key)
    if store is None:
        settings.chroma_dir.mkdir(parents=True, exist_ok=True)
        store = Chroma(
            collection_name="memories",
            embedding_function=get_embeddings(),
            persist_directory=str(settings.chroma_dir),
        )
        _MEMORY_VECTORSTORE_CACHE[key] = store
        logger.info("è®°å¿†å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    return store


def add_memory_to_vectorstore(
    memory_id: str,
    content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    settings: Optional[Settings] = None,
) -> None:
    """
    å°†è®°å¿†æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
    
    Args:
        memory_id: è®°å¿†ID
        content: è®°å¿†å†…å®¹
        memory_type: è®°å¿†ç±»å‹
        user_id: ç”¨æˆ·ID
        session_id: ä¼šè¯IDï¼ˆç”¨äºè®°å¿†éš”ç¦»ï¼‰
        settings: é…ç½®å¯¹è±¡
    """
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        
        # åˆ›å»º Document å¯¹è±¡
        metadata = {
            "memory_id": memory_id,
            "memory_type": memory_type,
            # æ˜¾å¼è®¾ç½® user_id å’Œ session_idï¼Œå³ä½¿ä¸º None
            # è¿™æ ·å¯ä»¥ç¡®ä¿å‘é‡åº“çš„ filter èƒ½æ­£ç¡®å·¥ä½œ
            "user_id": user_id if user_id else "",
            "session_id": session_id if session_id else "",
        }
        
        doc = Document(page_content=content, metadata=metadata)
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        vectorstore.add_documents([doc], ids=[memory_id])
        
        logger.debug(f"è®°å¿†å·²å‘é‡åŒ–: {memory_id}")
        
    except Exception as e:
        logger.error(f"å‘é‡åŒ–è®°å¿†å¤±è´¥ {memory_id}: {e}", exc_info=True)


def update_memory_in_vectorstore(
    memory_id: str,
    content: str,
    memory_type: str,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    settings: Optional[Settings] = None,
) -> None:
    """
    æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„è®°å¿†
    å…ˆåˆ é™¤æ—§å‘é‡ï¼Œå†æ·»åŠ æ–°å‘é‡
    """
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        
        # åˆ é™¤æ—§å‘é‡
        try:
            vectorstore.delete(ids=[memory_id])
        except Exception as e:
            logger.warning(f"åˆ é™¤æ—§å‘é‡å¤±è´¥: {e}")
        
        # æ·»åŠ æ–°å‘é‡
        add_memory_to_vectorstore(
            memory_id=memory_id,
            content=content,
            memory_type=memory_type,
            user_id=user_id,
            session_id=session_id,
            settings=settings,
        )
        
        logger.debug(f"è®°å¿†å‘é‡å·²æ›´æ–°: {memory_id}")
        
    except Exception as e:
        logger.error(f"æ›´æ–°è®°å¿†å‘é‡å¤±è´¥ {memory_id}: {e}", exc_info=True)


def delete_memory_from_vectorstore(
    memory_id: str,
    settings: Optional[Settings] = None,
) -> None:
    """ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤è®°å¿†"""
    try:
        if settings is None:
            from .config import get_settings
            settings = get_settings()
        
        vectorstore = get_memory_vectorstore(settings)
        vectorstore.delete(ids=[memory_id])
        
        logger.debug(f"è®°å¿†å‘é‡å·²åˆ é™¤: {memory_id}")
        
    except Exception as e:
        logger.warning(f"åˆ é™¤è®°å¿†å‘é‡å¤±è´¥: {e}")


def delete_memory_complete(
    session: Session,
    memory_id: str,
    settings: Optional[Settings] = None,
) -> bool:
    """
    å®Œæ•´åˆ é™¤å•æ¡è®°å¿†ï¼ˆæ•°æ®åº“ + å‘é‡åº“ï¼‰
    
    Returns:
        æ˜¯å¦åˆ é™¤æˆåŠŸ
    """
    from .database import delete_memory
    
    # å…ˆåˆ é™¤å‘é‡
    delete_memory_from_vectorstore(memory_id, settings)
    
    # å†åˆ é™¤æ•°æ®åº“è®°å½•
    success = delete_memory(session, memory_id)
    
    if success:
        logger.info(f"è®°å¿†å·²å®Œæ•´åˆ é™¤: {memory_id}")
    
    return success


# ==================== æ··åˆæ£€ç´¢æ¨¡å— ====================

def _retrieve_memories_by_vector(
    query: str,
    session: Session,
    settings: Settings,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    limit: int = 5,
) -> List[Memory]:
    """ä½¿ç”¨å‘é‡æ£€ç´¢è®°å¿†ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ï¼‰"""
    try:
        vectorstore = get_memory_vectorstore(settings)
        
        # æœç´¢å€™é€‰
        search_k = min(limit * 5, 100)
        
        try:
            # æ„å»ºè¿‡æ»¤æ¡ä»¶ï¼ˆä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤º Noneï¼Œç¡®ä¿èƒ½å¤Ÿè¿‡æ»¤ï¼‰
            filter_dict = {}
            if user_id:
                filter_dict["user_id"] = user_id
            if session_id:
                filter_dict["session_id"] = session_id
            
            # æ‰§è¡Œå‘é‡æœç´¢
            if filter_dict:
                results = vectorstore.similarity_search_with_score(
                    query,
                    k=search_k,
                    filter=filter_dict,
                )
            else:
                results = vectorstore.similarity_search_with_score(
                    query,
                    k=search_k,
                )
        except TypeError:
            # å¦‚æœä¸æ”¯æŒ filterï¼Œä½¿ç”¨æ—§æ–¹æ³•
            results = vectorstore.similarity_search_with_score(query, k=search_k)
        
        if not results:
            return []
        
        # ä»å‘é‡ç»“æœä¸­æå–è®°å¿†IDå’Œåˆ†æ•°
        memory_scores = {}
        filtered_count = 0
        for doc, distance in results:
            memory_id = doc.metadata.get("memory_id")
            if not memory_id:
                continue
            
            # è¿‡æ»¤ä¸åŒ¹é…çš„ç”¨æˆ·/ä¼šè¯ï¼ˆä½¿ç”¨ç©ºå­—ç¬¦ä¸²è¡¨ç¤º Noneï¼‰
            doc_user_id = doc.metadata.get("user_id", "")
            doc_session_id = doc.metadata.get("session_id", "")
            
            # å¦‚æœæŒ‡å®šäº† user_idï¼Œå¿…é¡»å®Œå…¨åŒ¹é…ï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ—§è®°å¿†æ²¡æœ‰ user_idï¼‰
            if user_id and doc_user_id != user_id:
                filtered_count += 1
                logger.debug(f"ğŸš« è¿‡æ»¤è®°å¿† {memory_id}ï¼šuser_id ä¸åŒ¹é…ï¼ˆè¦æ±‚={user_id}, å®é™…={doc_user_id}ï¼‰")
                continue
            
            # å¦‚æœæŒ‡å®šäº† session_idï¼Œå¿…é¡»å®Œå…¨åŒ¹é…ï¼ˆè¿™æ ·å¯ä»¥è¿‡æ»¤æ‰å…¶ä»–ä¼šè¯å’Œæ—§è®°å¿†ï¼‰
            if session_id and doc_session_id != session_id:
                filtered_count += 1
                logger.debug(f"ğŸš« è¿‡æ»¤è®°å¿† {memory_id}ï¼šsession_id ä¸åŒ¹é…ï¼ˆè¦æ±‚={session_id}, å®é™…={doc_session_id}ï¼‰")
                continue
            
            # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
            similarity_score = max(0.0, 1.0 - (distance / 2.0))
            memory_scores[memory_id] = similarity_score
        
        if filtered_count > 0:
            logger.info(f"âœ‚ï¸ å‘é‡æ£€ç´¢è¿‡æ»¤äº† {filtered_count} æ¡ä¸åŒ¹é…çš„è®°å¿†")
        
        if not memory_scores:
            logger.info(f"âŒ å‘é‡æ£€ç´¢æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å¿†ï¼ˆsession_id={session_id}ï¼‰")
            return []
        
        # ä»æ•°æ®åº“åŠ è½½è®°å¿†å¯¹è±¡
        memories = []
        for memory_id, score in list(memory_scores.items())[:limit * 2]:
            memory = get_memory_by_id(session, memory_id)
            if memory:
                # å­˜å‚¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                memory._vector_score = score
                memories.append(memory)
                logger.debug(f"âœ… å‘é‡æ£€ç´¢æ‰¾åˆ°è®°å¿†: {memory_id}, session_id={memory.session_id}, åˆ†æ•°={score:.3f}")
        
        return memories[:limit]
        
    except Exception as e:
        logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
        return []


async def retrieve_relevant_memories(
    session: Session,
    query: str,
    settings: Settings,
    user_id: Optional[str] = None,
    session_id: Optional[str] = None,
    max_memories: int = 5,
) -> List[Memory]:
    """
    æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„è®°å¿†ï¼ˆæ··åˆæ£€ç´¢ï¼‰
    
    å®ç°ä¸‰å±‚æ£€ç´¢ç­–ç•¥ï¼š
    1. å‘é‡æ£€ç´¢ï¼šä½¿ç”¨ Chroma è¿›è¡Œè¯­ä¹‰æœç´¢
    2. å…³é”®è¯æ£€ç´¢ï¼šSQL LIKE æŸ¥è¯¢ä½œä¸ºè¡¥å……
    3. é‡è¦è®°å¿†è¡¥å……ï¼šç¡®ä¿é«˜é‡è¦æ€§è®°å¿†ï¼ˆå¦‚å§“åï¼‰å§‹ç»ˆå¯ç”¨
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        query: æŸ¥è¯¢æ–‡æœ¬
        settings: é…ç½®å¯¹è±¡
        user_id: ç”¨æˆ·ID
        session_id: ä¼šè¯IDï¼ˆç”¨äºä¼šè¯éš”ç¦»ï¼‰
        max_memories: æœ€å¤§è¿”å›è®°å¿†æ•°
    
    Returns:
        ç›¸å…³è®°å¿†åˆ—è¡¨
    """
    # æ£€æŸ¥ä¼šè¯é…ç½®ï¼Œå†³å®šæ˜¯å¦å…±äº«è®°å¿†
    should_share = True
    if session_id:
        config = get_session_config(session, session_id)
        if config:
            should_share = config.share_memory
            logger.info(f"ğŸ”’ ä¼šè¯ {session_id} çš„è®°å¿†å…±äº«è®¾ç½®: share_memory={should_share}")
        else:
            # ä¼šè¯æ²¡æœ‰é…ç½®ï¼Œä»ç”¨æˆ·åå¥½ä¸­è¯»å–é»˜è®¤å€¼
            from .database import get_user_preferences
            prefs = get_user_preferences(session, user_id or "default")
            if prefs:
                should_share = prefs.default_share_memory
                logger.info(f"ğŸŒ ä¼šè¯ {session_id} æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·åå¥½: share_memory={should_share}")
            else:
                logger.warning(f"âš ï¸ ä¼šè¯ {session_id} æ²¡æœ‰é…ç½®ï¼Œä¸”æ²¡æœ‰ç”¨æˆ·åå¥½ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤: share_memory={should_share}")
    
    # å¦‚æœä¸å…±äº«è®°å¿†ï¼Œä½¿ç”¨ä¼šè¯éš”ç¦»
    effective_session_id = None if should_share else session_id
    
    if not should_share:
        logger.info(f"ğŸ” ä¼šè¯éš”ç¦»æ¨¡å¼ï¼šåªæ£€ç´¢ session_id={effective_session_id} çš„è®°å¿†")
    
    all_memories = {}
    
    # 1. å‘é‡æ£€ç´¢
    vector_memories = _retrieve_memories_by_vector(
        query=query,
        session=session,
        settings=settings,
        user_id=user_id,
        session_id=effective_session_id,
        limit=max_memories,
    )
    for mem in vector_memories:
        all_memories[mem.id] = mem
    
    # 2. å…³é”®è¯æ£€ç´¢ä½œä¸ºè¡¥å……
    keyword_memories = search_memories(
        session=session,
        query=query,
        user_id=user_id,
        session_id=effective_session_id,
        min_importance=50,
        limit=max_memories,
    )
    for mem in keyword_memories:
        if mem.id not in all_memories:
            mem._vector_score = 0.0  # æ²¡æœ‰å‘é‡åˆ†æ•°
            all_memories[mem.id] = mem
    
    # 3. è¡¥å……é‡è¦çš„ fact ç±»å‹è®°å¿†ï¼ˆå¦‚å§“åï¼‰
    if len(all_memories) < max_memories:
        important_memories = search_memories(
            session=session,
            memory_type="fact",
            user_id=user_id,
            session_id=effective_session_id,
            min_importance=80,
            limit=max_memories * 2,
        )
        for mem in important_memories:
            if mem.id not in all_memories and len(all_memories) < max_memories:
                mem._vector_score = 0.0
                all_memories[mem.id] = mem
    
    # æ›´æ–°è®¿é—®ç»Ÿè®¡
    for mem_id in all_memories:
        update_memory_access(session, mem_id)
    
    # ç»¼åˆè¯„åˆ†æ’åº
    sorted_memories = sorted(
        all_memories.values(),
        key=lambda m: (
            getattr(m, '_vector_score', 0.0) * 0.4 +  # å‘é‡ç›¸ä¼¼åº¦æƒé‡
            m.importance_score / 100 * 0.3 +  # é‡è¦æ€§æƒé‡
            min(m.access_count / 10, 1.0) * 0.1 +  # è®¿é—®é¢‘ç‡æƒé‡
            (1.0 if m.memory_type == "fact" else 0.5) * 0.2  # factç±»å‹ä¼˜å…ˆ
        ),
        reverse=True,
    )[:max_memories]
    
    logger.info(f"æ··åˆæ£€ç´¢æ‰¾åˆ° {len(sorted_memories)} æ¡ç›¸å…³è®°å¿†")
    return sorted_memories


# ==================== è®°å¿†ä¿å­˜ï¼ˆå«è‡ªåŠ¨å‘é‡åŒ–ï¼‰====================

async def save_conversation_and_extract_memories(
    session: Session,
    session_id: str,
    user_query: str,
    assistant_reply: str,
    settings: Settings,
    user_id: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> List[Memory]:
    """
    ä¿å­˜å¯¹è¯å¹¶è‡ªåŠ¨æå–è®°å¿†
    
    Returns:
        æ–°ä¿å­˜çš„è®°å¿†åˆ—è¡¨
    """
    # é¦–å…ˆä¿å­˜å¯¹è¯æ¶ˆæ¯åˆ°å†å²è®°å½•
    from .database import save_conversation_message
    
    try:
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        save_conversation_message(
            session=session,
            session_id=session_id,
            role="user",
            content=user_query,
            user_id=user_id,
            metadata=metadata,
        )
        
        # ä¿å­˜åŠ©æ‰‹å›å¤
        save_conversation_message(
            session=session,
            session_id=session_id,
            role="assistant",
            content=assistant_reply,
            user_id=user_id,
            metadata=metadata,
        )
        
        logger.debug(f"å·²ä¿å­˜å¯¹è¯åˆ°ä¼šè¯ {session_id}")
    except Exception as e:
        logger.error(f"ä¿å­˜å¯¹è¯æ¶ˆæ¯å¤±è´¥: {e}", exc_info=True)
    
    # æ£€æŸ¥ä¼šè¯é…ç½®
    config = get_session_config(session, session_id)
    if config and not config.auto_extract:
        logger.debug("ä¼šè¯ç¦ç”¨äº†è‡ªåŠ¨æå–ï¼Œè·³è¿‡è®°å¿†æå–")
        return []
    
    # æ„å»ºå¯¹è¯æ–‡æœ¬
    conversation_text = f"ç”¨æˆ·: {user_query}\nåŠ©æ‰‹: {assistant_reply}"
    
    # æå–è®°å¿†
    extracted_memories = await extract_memories_from_conversation(
        conversation_text=conversation_text,
        settings=settings,
        session_id=session_id,
        user_id=user_id,
    )
    
    # ä¿å­˜æå–çš„è®°å¿†ï¼ˆä½¿ç”¨å»é‡å’Œåˆå¹¶é€»è¾‘ï¼Œå¹¶è‡ªåŠ¨å‘é‡åŒ–ï¼‰
    saved_memories = []
    for mem in extracted_memories:
        try:
            # ä½¿ç”¨å»é‡ä¿å­˜ï¼ˆé™ä½é˜ˆå€¼ï¼Œæé«˜å»é‡çµæ•åº¦ï¼‰
            memory_record = save_memory_with_dedup(
                session=session,
                content=mem["content"],
                memory_type=mem["type"],
                importance_score=mem["importance"],
                user_id=user_id,
                session_id=session_id,
                metadata={"extracted_at": "auto", **(metadata or {})},
                threshold=0.65,  # ğŸ”§ ä» 0.75 é™ä½åˆ° 0.65ï¼Œæé«˜å»é‡çµæ•åº¦
            )
            
            # å‘é‡åŒ–
            add_memory_to_vectorstore(
                memory_id=memory_record.id,
                content=memory_record.content,
                memory_type=memory_record.memory_type,
                user_id=memory_record.user_id,
                session_id=memory_record.session_id,
                settings=settings,
            )
            
            saved_memories.append(memory_record)
            
        except Exception as e:
            logger.error(f"ä¿å­˜è®°å¿†å¤±è´¥: {e}", exc_info=True)
    
    logger.info(f"æˆåŠŸä¿å­˜ {len(saved_memories)} æ¡è®°å¿†")
    return saved_memories


# ==================== è®°å¿†æ ¼å¼åŒ–æ¨¡å— ====================

def format_memories_for_prompt(memories: List[Memory]) -> str:
    """
    å°†è®°å¿†åˆ—è¡¨æ ¼å¼åŒ–ä¸º LLM promptï¼ˆéšå¼æ ¼å¼ï¼‰
    ä¸æ˜¾ç¤º"è®°å¿†"ã€"ä¿¡æ¯"ç­‰æ ‡ç­¾ï¼Œåªæä¾›çº¯å†…å®¹
    """
    if not memories:
        return ""
    
    return "\n".join(mem.content for mem in memories)













