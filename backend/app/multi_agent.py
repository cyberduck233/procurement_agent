"""
å¤šæ™ºèƒ½ä½“åè°ƒå™¨ï¼ˆMulti-Agent Orchestratorï¼‰
è´Ÿè´£ä»»åŠ¡åˆ†è§£ã€æ™ºèƒ½ä½“é€‰æ‹©ã€åè°ƒæ‰§è¡Œå’Œç»“æœæ±‡æ€»
"""
from __future__ import annotations

import logging
import uuid
from datetime import datetime
from typing import Any, AsyncGenerator, Dict, List, Optional

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from sqlalchemy.orm import Session

from .agent_roles import (
    AGENT_REGISTRY,
    analysis_specialist_node,
    retrieval_specialist_node,
    summarization_specialist_node,
    verification_specialist_node,
    mysql_specialist_node,
)
from .config import Settings
from .database import ToolRecord
from .graph_agent import invoke_llm, parse_json_from_llm
from .shared_workspace import (
    AgentMessage,
    MultiAgentState,
    SharedWorkspace,
    SubTask,
    create_initial_multi_agent_state,
)

logger = logging.getLogger(__name__)


# ==================== åè°ƒå™¨èŠ‚ç‚¹å‡½æ•° ====================

async def orchestrator_planner_node(
    state: MultiAgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    åè°ƒå™¨ - è§„åˆ’èŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. ç†è§£ç”¨æˆ·æ„å›¾
    2. åˆ†è§£ä»»åŠ¡ä¸ºå­ä»»åŠ¡
    3. é€‰æ‹©åˆé€‚çš„æ™ºèƒ½ä½“
    4. åˆ¶å®šæ‰§è¡Œç­–ç•¥
    """
    logger.info("ğŸ¯ [åè°ƒå™¨-è§„åˆ’] å¼€å§‹ä»»åŠ¡åˆ†è§£...")
    
    workspace = SharedWorkspace(state)
    user_query = state.get("user_query", "")
    
    # ä½¿ç”¨ LLM è¿›è¡Œæ™ºèƒ½ä»»åŠ¡åˆ†è§£
    planning_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åè°ƒå™¨ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¶å®šæ‰§è¡Œè®¡åˆ’ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å¯ç”¨çš„æ™ºèƒ½ä½“ï¼š
1. **æ£€ç´¢ä¸“å®¶**ï¼ˆretrieval_specialistï¼‰- çŸ¥è¯†åº“æ£€ç´¢ã€ç½‘ç»œæœç´¢ã€ä¿¡æ¯æ”¶é›†
2. **åˆ†æä¸“å®¶**ï¼ˆanalysis_specialistï¼‰- æ·±åº¦åˆ†æã€å†…å®¹ç†è§£ã€æ•°æ®æå–
3. **æ€»ç»“ä¸“å®¶**ï¼ˆsummarization_specialistï¼‰- ä¿¡æ¯æ•´åˆã€æŠ¥å‘Šç”Ÿæˆã€ç­”æ¡ˆåˆæˆ
4. **éªŒè¯ä¸“å®¶**ï¼ˆverification_specialistï¼‰- è´¨é‡æ£€æŸ¥ã€äº‹å®æ ¸æŸ¥ï¼ˆå¯é€‰ï¼Œä»…ç”¨äºé«˜è´¨é‡è¦æ±‚åœºæ™¯ï¼‰
5. **MySQLä¸“å®¶**ï¼ˆmysql_specialistï¼‰- æ•°æ®åº“æŸ¥è¯¢ã€SQLç”Ÿæˆã€æ•°æ®åˆ†æ  
è¯·ç”Ÿæˆä»»åŠ¡åˆ†è§£è®¡åˆ’ï¼Œä»¥ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "task_type": "ä¿¡æ¯æ£€ç´¢|æ•°æ®åˆ†æ|æ•°æ®åº“æŸ¥è¯¢|å†…å®¹åˆ›ä½œ|å¤åˆä»»åŠ¡",
  "complexity": "ç®€å•|ä¸­ç­‰|å¤æ‚",
  "sub_tasks": [
    {{
      "task_id": "task_1",
      "task_type": "retrieval|analysis|summarization|verification|mysql_query",
      "description": "è¯¦ç»†çš„ä»»åŠ¡æè¿°ï¼ŒåŒ…æ‹¬å…·ä½“ç›®æ ‡å’Œé¢„æœŸè¾“å‡º",
      "assigned_agent": "agent_id",
      "depends_on": []  // ä¾èµ–çš„å‰ç½®ä»»åŠ¡IDåˆ—è¡¨ï¼ˆç©ºæ•°ç»„è¡¨ç¤ºæ— ä¾èµ–ï¼‰
    }},
    ...
  ],
  "execution_mode": "sequential|parallel",
  "reasoning": "ä»»åŠ¡åˆ†è§£çš„ç†ç”±"
}}

ã€é‡è¦è§„åˆ™ - å¿…é¡»éµå®ˆã€‘ï¼š
1. **ä»»åŠ¡ä¾èµ–å…³ç³»å¿…é¡»æ­£ç¡®**ï¼š
   - æ£€ç´¢ä¸“å®¶ï¼ˆretrieval_specialistï¼‰é€šå¸¸æ˜¯ç¬¬ä¸€æ­¥ï¼Œæ— ä¾èµ–
   - MySQLä¸“å®¶ï¼ˆmysql_specialistï¼‰é€šå¸¸ç‹¬ç«‹æ‰§è¡Œï¼Œæˆ–ä¾èµ–äºéœ€æ±‚åˆ†æ
   - åˆ†æä¸“å®¶ï¼ˆanalysis_specialistï¼‰å¯ä¾èµ–æ£€ç´¢ä¸“å®¶æˆ–MySQLä¸“å®¶çš„ç»“æœ
   - æ€»ç»“ä¸“å®¶ï¼ˆsummarization_specialistï¼‰ä¾èµ–æ‰€æœ‰å‰ç½®åˆ†æä»»åŠ¡
   - éªŒè¯ä¸“å®¶ï¼ˆverification_specialistï¼‰å¿…é¡»ä¾èµ–æ€»ç»“ä¸“å®¶

2. **MySQLä¸“å®¶ä½¿ç”¨è§„åˆ™**ï¼š
   - åªåœ¨æ¶‰åŠæ•°æ®åº“æŸ¥è¯¢ã€æ•°æ®ç»Ÿè®¡ã€æ•°æ®åˆ†æç­‰åœºæ™¯ä½¿ç”¨
   - å…³é”®è¯ï¼šæŸ¥è¯¢æ•°æ®åº“ã€ç»Ÿè®¡ã€æ•°æ®åˆ†æã€SQLã€è¡¨ã€è®°å½•ç­‰
   - MySQLä¸“å®¶å¯ä»¥ä¸æ£€ç´¢ä¸“å®¶å¹¶è¡Œæ‰§è¡Œ
   - å¦‚æœé—®é¢˜æ—¢éœ€è¦æ•°æ®åº“æŸ¥è¯¢åˆéœ€è¦çŸ¥è¯†åº“æ£€ç´¢ï¼Œä¸¤è€…å¯å¹¶è¡Œ
   
3. **éªŒè¯ä¸“å®¶ä½¿ç”¨è§„åˆ™**ï¼š
   - åªåœ¨ä»¥ä¸‹åœºæ™¯ä½¿ç”¨éªŒè¯ä¸“å®¶ï¼š
     * ç”¨æˆ·æ˜ç¡®è¦æ±‚"éªŒè¯"ã€"æ£€æŸ¥"ã€"ç¡®ä¿å‡†ç¡®"
     * æ¶‰åŠäº‹å®æ€§ä¿¡æ¯ã€æ•°æ®æŠ¥å‘Šã€ç ”ç©¶æŠ¥å‘Šç­‰é«˜è´¨é‡è¦æ±‚åœºæ™¯
   - å¦‚æœä½¿ç”¨éªŒè¯ä¸“å®¶ï¼Œå¿…é¡»æ”¾åœ¨æœ€åï¼Œä¸”ä¾èµ–æ€»ç»“ä¸“å®¶ï¼š
     {{"task_id": "task_final", "assigned_agent": "verification_specialist", "depends_on": ["summary_task_id"]}}

4. **æ‰§è¡Œé¡ºåºç¤ºä¾‹**ï¼ˆæ•°æ®åº“æŸ¥è¯¢+åˆ†æä»»åŠ¡ï¼‰ï¼š
   task_1: mysql_specialist (æ— ä¾èµ–ï¼Œæ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢)
   task_2: analysis_specialist (ä¾èµ– task_1ï¼Œåˆ†ææŸ¥è¯¢ç»“æœ)
   task_3: summarization_specialist (ä¾èµ– task_2)

5. **é—®ç­”æµç¨‹**ï¼ˆâ­é‡è¦ï¼‰ï¼š
   - **ä¸€èˆ¬é—®é¢˜**ï¼ˆéœ€è¦åˆ†æä½†ä¸éœ€æ·±åº¦ç ”ç©¶ï¼‰ï¼šæ£€ç´¢ä¸“å®¶ â†’MySQLä¸“å®¶ â†’ åˆ†æä¸“å®¶ â†’ æ€»ç»“ä¸“å®¶ï¼ˆ4ä¸ªæ™ºèƒ½ä½“ï¼‰
   - **æ·±å…¥ç ”ç©¶**ï¼ˆå¤šè§’åº¦ã€æ·±åº¦åˆ†æã€è´¨é‡è¦æ±‚é«˜ï¼‰ï¼šæ£€ç´¢ä¸“å®¶ â†’ MySQLä¸“å®¶ â†’å¤šä¸ªåˆ†æä¸“å®¶ â†’ æ€»ç»“ä¸“å®¶ â†’ éªŒè¯ä¸“å®¶ï¼ˆ5-6ä¸ªæ™ºèƒ½ä½“ï¼‰

6. **ä¼˜å…ˆä½¿ç”¨ç®€åŒ–æµç¨‹**ï¼š
   - å¦‚æœé—®é¢˜å¯ä»¥ç›´æ¥å›ç­”ï¼Œä¸è¦ä½¿ç”¨æ£€ç´¢ä¸“å®¶
   - ä¼˜å…ˆ2-3ä¸ªæ™ºèƒ½ä½“çš„æµç¨‹
   - åªåœ¨å¿…è¦æ—¶ä½¿ç”¨éªŒè¯ä¸“å®¶

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
    
    try:
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": planning_prompt}],
            settings=settings,
            temperature=0.3,
            max_tokens=1500,
        )
        
        plan_data = parse_json_from_llm(llm_response)
        
        task_type = plan_data.get("task_type", "å¤åˆä»»åŠ¡")
        complexity = plan_data.get("complexity", "ä¸­ç­‰")
        sub_tasks_data = plan_data.get("sub_tasks", [])
        execution_mode = plan_data.get("execution_mode", "sequential")
        reasoning = plan_data.get("reasoning", "")
        
        logger.info(
            f"ğŸ“‹ ä»»åŠ¡åˆ†è§£å®Œæˆï¼š{task_type}ï¼ˆ{complexity}ï¼‰ï¼Œ"
            f"{len(sub_tasks_data)} ä¸ªå­ä»»åŠ¡ï¼Œ{execution_mode} æ‰§è¡Œ"
        )
        
        # åˆ›å»ºå­ä»»åŠ¡å¯¹è±¡
        sub_tasks = []
        for i, task_data in enumerate(sub_tasks_data):
            subtask = SubTask(
                task_id=task_data.get("task_id", f"task_{i+1}"),
                task_type=task_data.get("task_type", "unknown"),
                description=task_data.get("description", ""),
                assigned_agent=task_data.get("assigned_agent", ""),
            )
            workspace.add_subtask(subtask)
            sub_tasks.append(subtask)
        
        # ç”Ÿæˆè®¡åˆ’æ‘˜è¦
        plan_summary = f"""ä»»åŠ¡åˆ†è§£è®¡åˆ’ï¼š
ç±»å‹ï¼š{task_type}
å¤æ‚åº¦ï¼š{complexity}
æ‰§è¡Œæ¨¡å¼ï¼š{execution_mode}
å­ä»»åŠ¡æ•°ï¼š{len(sub_tasks)}

å­ä»»åŠ¡åˆ—è¡¨ï¼š
{chr(10).join(f"{i+1}. [{t.assigned_agent}] {t.description}" for i, t in enumerate(sub_tasks))}

ç†ç”±ï¼š{reasoning}
"""
        
        return {
            "orchestrator_plan": plan_summary,
            "execution_mode": execution_mode,
            "thoughts": [f"åè°ƒå™¨å®Œæˆä»»åŠ¡åˆ†è§£ï¼š{len(sub_tasks)} ä¸ªå­ä»»åŠ¡"],
            "observations": [f"æ‰§è¡Œè®¡åˆ’å·²ç”Ÿæˆï¼Œæ¨¡å¼ï¼š{execution_mode}"],
        }
    
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡åˆ†è§£å¤±è´¥: {e}", exc_info=True)
        
        # é™çº§ï¼šä½¿ç”¨ç®€å•çš„é»˜è®¤è®¡åˆ’
        default_subtasks = [
            SubTask(
                task_id="task_1",
                task_type="retrieval",
                description="æ£€ç´¢ç›¸å…³ä¿¡æ¯",
                assigned_agent="retrieval_specialist",
            ),
            SubTask(
                task_id="task_2",
                task_type="summarization",
                description="ç”Ÿæˆæœ€ç»ˆå›ç­”",
                assigned_agent="summarization_specialist",
            ),
        ]
        
        for task in default_subtasks:
            workspace.add_subtask(task)
        
        return {
            "orchestrator_plan": "ä½¿ç”¨é»˜è®¤æ‰§è¡Œè®¡åˆ’ï¼ˆ2ä¸ªå­ä»»åŠ¡ï¼‰",
            "execution_mode": "sequential",
            "thoughts": [f"ä½¿ç”¨é»˜è®¤è®¡åˆ’ï¼ˆè§„åˆ’å¤±è´¥ï¼š{str(e)[:50]}ï¼‰"],
        }


async def orchestrator_executor_node(
    state: MultiAgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """
    åè°ƒå™¨ - æ‰§è¡ŒèŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. æ ¹æ®è®¡åˆ’é¡ºåºæ‰§è¡Œå­ä»»åŠ¡
    2. è°ƒåº¦æ™ºèƒ½ä½“æ‰§è¡Œ
    3. ç›‘æ§æ‰§è¡ŒçŠ¶æ€
    """
    logger.info("âš™ï¸ [åè°ƒå™¨-æ‰§è¡Œ] å¼€å§‹åè°ƒæ™ºèƒ½ä½“æ‰§è¡Œ...")
    
    workspace = SharedWorkspace(state)
    execution_mode = state.get("execution_mode", "sequential")
    
    # è·å–å¾…æ‰§è¡Œçš„å­ä»»åŠ¡
    pending_tasks = workspace.get_pending_subtasks()
    
    if not pending_tasks:
        logger.info("âœ… æ‰€æœ‰å­ä»»åŠ¡å·²å®Œæˆ")
        return {
            "thoughts": ["æ‰€æœ‰å­ä»»åŠ¡å·²æ‰§è¡Œå®Œæˆ"],
            "next_action": "complete",
        }
    
    # è·å–ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ä»»åŠ¡
    next_task = pending_tasks[0]
    
    logger.info(
        f"ğŸ“Œ æ‰§è¡Œå­ä»»åŠ¡: {next_task.task_id} "
        f"[{next_task.assigned_agent}] - {next_task.description}"
    )
    
    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    workspace.update_subtask_status(next_task.task_id, "in_progress")
    
    # å‘é€ä»»åŠ¡åˆ†é…æ¶ˆæ¯
    workspace.send_message(
        from_agent="orchestrator",
        to_agent=next_task.assigned_agent,
        message_type="task_request",
        content={
            "task_id": next_task.task_id,
            "description": next_task.description,
        },
    )
    
    return {
        "thoughts": [f"è°ƒåº¦æ™ºèƒ½ä½“æ‰§è¡Œ: {next_task.assigned_agent}"],
        "observations": [f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {next_task.description}"],
        "next_action": f"execute_{next_task.assigned_agent}",
    }


async def orchestrator_aggregator_node(
    state: MultiAgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    åè°ƒå™¨ - æ±‡æ€»èŠ‚ç‚¹
    
    èŒè´£ï¼š
    1. æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»“æœ
    2. æ•´åˆå’Œæ±‡æ€»
    3. ç”Ÿæˆæœ€ç»ˆè¾“å‡º
    """
    logger.info("ğŸ“Š [åè°ƒå™¨-æ±‡æ€»] å¼€å§‹æ±‡æ€»æ‰€æœ‰ç»“æœ...")
    
    workspace = SharedWorkspace(state)
    
    # è·å–æœ€ç»ˆç­”æ¡ˆï¼ˆç”±æ€»ç»“ä¸“å®¶ç”Ÿæˆï¼‰
    final_answer = workspace.get_shared_data("final_answer", "")
    
    if not final_answer:
        # å¦‚æœæ²¡æœ‰æ€»ç»“ä¸“å®¶çš„ç»“æœï¼Œä»å…¶ä»–æ™ºèƒ½ä½“ç»“æœä¸­æå–
        retrieval_results = workspace.get_shared_data("retrieval_results", {})
        analysis_result = workspace.get_shared_data("analysis_result", {})
        
        if retrieval_results or analysis_result:
            final_answer = "å·²æ”¶é›†ç›¸å…³ä¿¡æ¯ï¼Œä½†æœªç”Ÿæˆå®Œæ•´æ€»ç»“ã€‚"
        else:
            final_answer = "æœªèƒ½æ”¶é›†åˆ°æœ‰æ•ˆç»“æœã€‚"
    
    # è·å–è´¨é‡è¯„åˆ†
    verification_result = workspace.get_shared_data("verification_result", {})
    quality_score = verification_result.get("overall_score", 7) / 10.0
    
    # æ”¶é›†æ‰€æœ‰æ™ºèƒ½ä½“çš„æ€è€ƒå’Œè§‚å¯Ÿ
    all_thoughts = []
    all_observations = []
    
    agent_thoughts = state.get("agent_thoughts", {})
    agent_observations = state.get("agent_observations", {})
    
    for agent_id, thoughts in agent_thoughts.items():
        agent_name = AGENT_REGISTRY.get(agent_id, {}).get("name", agent_id)
        all_thoughts.extend([f"[{agent_name}] {t}" for t in thoughts])
    
    for agent_id, observations in agent_observations.items():
        agent_name = AGENT_REGISTRY.get(agent_id, {}).get("name", agent_id)
        all_observations.extend([f"[{agent_name}] {o}" for o in observations])
    
    logger.info(f"âœ… æ±‡æ€»å®Œæˆï¼Œæœ€ç»ˆç­”æ¡ˆé•¿åº¦ï¼š{len(final_answer)} å­—ç¬¦")
    
    return {
        "final_answer": final_answer,
        "quality_score": quality_score,
        "is_complete": True,
        "thoughts": all_thoughts,
        "observations": all_observations,
    }


# ==================== è·¯ç”±å‡½æ•° ====================

def route_after_planning(state: MultiAgentState) -> str:
    """è§„åˆ’åçš„è·¯ç”±"""
    return "executor"


def route_after_execution(state: MultiAgentState) -> str:
    """æ‰§è¡Œåçš„è·¯ç”±"""
    next_action = state.get("next_action", "")
    
    if next_action == "complete":
        return "aggregator"
    
    # æ ¹æ® next_action è·¯ç”±åˆ°å…·ä½“çš„æ™ºèƒ½ä½“
    if next_action.startswith("execute_"):
        agent_id = next_action.replace("execute_", "")
        return agent_id
    
    return "aggregator"


def route_after_agent(state: MultiAgentState) -> str:
    """æ™ºèƒ½ä½“æ‰§è¡Œåçš„è·¯ç”±"""
    workspace = SharedWorkspace(state)
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å­ä»»åŠ¡éƒ½å®Œæˆ
    if workspace.all_subtasks_completed():
        return "aggregator"
    
    # ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå­ä»»åŠ¡
    return "executor"


def should_end(state: MultiAgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»“æŸ"""
    is_complete = state.get("is_complete", False)
    return END if is_complete else "continue"


# ==================== å¤šæ™ºèƒ½ä½“å·¥ä½œæµæ„å»º ====================

def create_multi_agent_graph(
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> StateGraph:
    """
    åˆ›å»ºå¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµ
    
    å·¥ä½œæµï¼š
    1. åè°ƒå™¨è§„åˆ’ -> ä»»åŠ¡åˆ†è§£
    2. åè°ƒå™¨æ‰§è¡Œ -> è°ƒåº¦æ™ºèƒ½ä½“
    3. æ™ºèƒ½ä½“æ‰§è¡Œ -> å„ä¸“å®¶æ‰§è¡Œä»»åŠ¡
    4. åè°ƒå™¨æ±‡æ€» -> æ•´åˆç»“æœ
    """
    logger.info("ğŸ—ï¸ æ„å»ºå¤šæ™ºèƒ½ä½“åä½œå·¥ä½œæµ...")
    
    workflow = StateGraph(MultiAgentState)
    
    # ========== åè°ƒå™¨èŠ‚ç‚¹ ==========
    
    async def planner_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await orchestrator_planner_node(state, settings)
    
    async def executor_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await orchestrator_executor_node(state, settings, session, tool_records)
    
    async def aggregator_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await orchestrator_aggregator_node(state, settings)
    
    workflow.add_node("planner", planner_wrapper)
    workflow.add_node("executor", executor_wrapper)
    workflow.add_node("aggregator", aggregator_wrapper)
    
    # ========== æ™ºèƒ½ä½“èŠ‚ç‚¹ ==========
    
    async def retrieval_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await retrieval_specialist_node(state, settings, session, tool_records)
    
    async def analysis_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await analysis_specialist_node(state, settings, session)
    
    async def summarization_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await summarization_specialist_node(state, settings, session, tool_records=tool_records)
    
    async def verification_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await verification_specialist_node(state, settings, session)

    async def mysql_wrapper(state: MultiAgentState) -> Dict[str, Any]:
        return await mysql_specialist_node(state, settings, session)

    workflow.add_node("mysql_specialist", mysql_wrapper)
    workflow.add_node("retrieval_specialist", retrieval_wrapper)
    workflow.add_node("analysis_specialist", analysis_wrapper)
    workflow.add_node("summarization_specialist", summarization_wrapper)
    workflow.add_node("verification_specialist", verification_wrapper)
    
    # ========== è®¾ç½®æµç¨‹ ==========
    
    workflow.set_entry_point("planner")
    
    # è§„åˆ’ -> æ‰§è¡Œ
    workflow.add_edge("planner", "executor")
    
    # æ‰§è¡Œ -> è·¯ç”±åˆ°æ™ºèƒ½ä½“æˆ–æ±‡æ€»
    workflow.add_conditional_edges(
        "executor",
        route_after_execution,
        {
            "retrieval_specialist": "retrieval_specialist",
            "analysis_specialist": "analysis_specialist",
            "summarization_specialist": "summarization_specialist",
            "verification_specialist": "verification_specialist",
            "mysql_specialist": "mysql_specialist",
            "aggregator": "aggregator",
        },
    )
    
    # æ™ºèƒ½ä½“ -> ç»§ç»­æ‰§è¡Œæˆ–æ±‡æ€»
    for agent_id in ["retrieval_specialist", "analysis_specialist", "summarization_specialist", "verification_specialist","mysql_specialist"]:
        workflow.add_conditional_edges(
            agent_id,
            route_after_agent,
            {
                "executor": "executor",
                "aggregator": "aggregator",
            },
        )
    
    # æ±‡æ€» -> ç»“æŸ
    workflow.add_edge("aggregator", END)
    
    logger.info("âœ… å¤šæ™ºèƒ½ä½“å·¥ä½œæµæ„å»ºå®Œæˆ")
    
    return workflow


# ==================== æ‰§è¡Œå‡½æ•° ====================

async def run_multi_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = True,
    conversation_history: Optional[List[Dict[str, str]]] = None,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    execution_mode: str = "sequential",
) -> Dict[str, Any]:
    """
    è¿è¡Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    
    Args:
        user_query: ç”¨æˆ·æŸ¥è¯¢
        settings: é…ç½®
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
        use_knowledge_base: æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“
        conversation_history: å¯¹è¯å†å²
        session_id: ä¼šè¯ID
        user_id: ç”¨æˆ·ID
        execution_mode: æ‰§è¡Œæ¨¡å¼ï¼ˆsequential æˆ– parallelï¼‰
    
    Returns:
        æ‰§è¡Œç»“æœ
    """
    logger.info(f"ğŸš€ å¯åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¤„ç†é—®é¢˜: {user_query}")
    
    if not session_id:
        session_id = str(uuid.uuid4())
    
    # æ„å»ºå·¥ä½œæµ
    workflow = create_multi_agent_graph(settings, session, tool_records)
    
    # ç¼–è¯‘å›¾
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = create_initial_multi_agent_state(
        user_query=user_query,
        conversation_history=conversation_history,
        session_id=session_id,
        user_id=user_id,
        use_knowledge_base=use_knowledge_base,
        available_tools=[tool.id for tool in tool_records],
        execution_mode=execution_mode,
    )
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æ‰§è¡Œå·¥ä½œæµ
    try:
        final_state = await app.ainvoke(initial_state, config=config)
        
        logger.info("âœ… å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ‰§è¡Œå®Œæˆ")
        
        return {
            "success": True,
            "final_answer": final_state.get("final_answer", "æœªèƒ½ç”Ÿæˆç­”æ¡ˆ"),
            "orchestrator_plan": final_state.get("orchestrator_plan", ""),
            "sub_tasks": final_state.get("sub_tasks", []),
            "agent_results": final_state.get("agent_results", {}),
            "thoughts": final_state.get("thoughts", []),
            "observations": final_state.get("observations", []),
            "quality_score": final_state.get("quality_score", 0.0),
            "thread_id": thread_id,
            "session_id": session_id,
        }
    
    except Exception as e:
        logger.error(f"âŒ å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "final_answer": f"ç³»ç»Ÿæ‰§è¡Œå‡ºé”™ï¼š{str(e)}",
            "error": str(e),
        }


async def stream_multi_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = True,
    conversation_history: Optional[List[Dict[str, str]]] = None,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    execution_mode: str = "sequential",
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    æµå¼è¿è¡Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
    
    ç”¨äºå‰ç«¯å®æ—¶å±•ç¤ºå„æ™ºèƒ½ä½“çš„æ‰§è¡Œè¿‡ç¨‹
    """
    logger.info(f"ğŸŒŠ å¯åŠ¨æµå¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ: {user_query}")
    
    if not session_id:
        session_id = str(uuid.uuid4())
    
    workflow = create_multi_agent_graph(settings, session, tool_records)
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    initial_state = create_initial_multi_agent_state(
        user_query=user_query,
        conversation_history=conversation_history,
        session_id=session_id,
        user_id=user_id,
        use_knowledge_base=use_knowledge_base,
        available_tools=[tool.id for tool in tool_records],
        execution_mode=execution_mode,
    )
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æµå¼æ‰§è¡Œ
    async for event in app.astream(initial_state, config=config):
        for node_name, node_output in event.items():
            if node_name != "__end__":
                # åˆ¤æ–­èŠ‚ç‚¹ç±»å‹
                if node_name == "planner":
                    event_type = "orchestrator_plan"
                elif node_name == "executor":
                    event_type = "orchestrator_execute"
                elif node_name == "aggregator":
                    event_type = "orchestrator_aggregate"
                elif node_name in AGENT_REGISTRY:
                    event_type = "agent_execution"
                else:
                    event_type = "node_output"
                
                yield {
                    "event": event_type,
                    "node": node_name,
                    "data": node_output,
                    "timestamp": datetime.now().isoformat(),
                }
    
    # å®Œæˆ
    yield {
        "event": "completed",
        "thread_id": thread_id,
        "timestamp": datetime.now().isoformat(),
    }

