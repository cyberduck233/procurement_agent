"""
LangGraph Agent - å®Œæ•´çš„æ™ºèƒ½ä½“å®ç°
æ”¯æŒï¼šå¤šæ­¥éª¤è§„åˆ’ã€å¹¶è¡Œæ‰§è¡Œã€æ¡ä»¶è·¯ç”±ã€çŠ¶æ€æŒä¹…åŒ–ã€äººå·¥ä»‹å…¥
"""
from __future__ import annotations

import json
import logging
import operator
import re
import uuid
import asyncio
from datetime import datetime
from typing import Annotated, Any, Dict, List, Literal, Optional, Sequence, TypedDict, AsyncGenerator

import httpx
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.prebuilt import ToolNode
from sqlalchemy.orm import Session

from .config import Settings
from .database import ToolRecord, get_session_factory
from .rag_service import retrieve_context
from .tool_service import execute_tool, parse_tool_call
from .memory_service import (
    retrieve_relevant_memories,
    format_memories_for_prompt,
    save_conversation_and_extract_memories,
)

logger = logging.getLogger(__name__)


# ==================== LLM è°ƒç”¨å·¥å…· ====================

async def invoke_llm(
    messages: List[Dict[str, str]],
    settings: Settings,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
) -> tuple[str, Dict[str, Any]]:
    """
    è°ƒç”¨ DeepSeek API è¿›è¡Œæ¨ç†
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        settings: é…ç½®å¯¹è±¡
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
    
    Returns:
        (å›å¤å†…å®¹, å®Œæ•´å“åº”æ•°æ®)
    """
    payload: Dict[str, Any] = {
        "model": "deepseek-r1",
        "messages": messages,
        "temperature": temperature,
        "stream": False,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens

    headers = {
        "Authorization": f"Bearer {settings.deepseek_api_key}",
        "Content-Type": "application/json",
    }
    endpoint = f"{settings.deepseek_base_url.rstrip('/')}/chat/completions"

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0)) as client:  # å¢åŠ åˆ°120ç§’
            response = await client.post(endpoint, json=payload, headers=headers)

        if response.status_code != 200:
            logger.error(
                "DeepSeek API error %s: %s", response.status_code, response.text
            )
            return f"API è°ƒç”¨å¤±è´¥: {response.status_code}", {}

        data = response.json()
        reply = data["choices"][0]["message"]["content"]
        return reply, data
    
    except httpx.TimeoutException as e:
        logger.error(f"LLM è°ƒç”¨è¶…æ—¶ï¼ˆ120ç§’ï¼‰: {e}")
        return f"LLM è°ƒç”¨è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•", {}
    except Exception as e:
        logger.error(f"LLM è°ƒç”¨å¼‚å¸¸: {e}", exc_info=True)
        return f"LLM è°ƒç”¨å¤±è´¥: {str(e)}", {}


async def stream_llm(
    messages: List[Dict[str, str]],
    settings: Settings,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
) -> AsyncGenerator[str, None]:
    """
    æµå¼è°ƒç”¨ DeepSeek API
    """
    payload: Dict[str, Any] = {
        "model": "deepseek-r1",
        "messages": messages,
        "temperature": temperature,
        "stream": True,
    }
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens

    headers = {
        "Authorization": f"Bearer {settings.deepseek_api_key}",
        "Content-Type": "application/json",
    }
    endpoint = f"{settings.deepseek_base_url.rstrip('/')}/chat/completions"

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(120.0)) as client:
            async with client.stream("POST", endpoint, json=payload, headers=headers) as response:
                if response.status_code != 200:
                    yield f"API Error: {response.status_code}"
                    return

                async for line in response.aiter_lines():
                    if not line.strip():
                        continue
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str == "[DONE]":
                            break
                        try:
                            data = json.loads(data_str)
                            content = data["choices"][0]["delta"].get("content", "")
                            if content:
                                yield content
                        except:
                            pass
    except Exception as e:
        logger.error(f"LLM Stream Error: {e}")
        yield f"Error: {str(e)}"


def parse_json_from_llm(text: str) -> Dict[str, Any]:
    """
    ä» LLM å“åº”ä¸­æå– JSON
    æ”¯æŒå¤„ç† markdown ä»£ç å—åŒ…è£¹çš„ JSON
    """
    # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    text = text.strip()
    if text.startswith("```json"):
        text = text[7:]
    elif text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]
    
    text = text.strip()
    
    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        logger.warning(f"JSON è§£æå¤±è´¥: {e}, åŸå§‹æ–‡æœ¬: {text[:200]}")
        # å®½å®¹è§£æï¼šå°è¯•æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªå¯èƒ½å®Œæ•´çš„å¯¹è±¡
        try:
            end_idx = max(text.rfind('}'), text.rfind(']'))
            if end_idx != -1:
                truncated = text[:end_idx+1]
                return json.loads(truncated)
        except Exception:
            pass
        # è¿”å›é»˜è®¤ç»“æ„
        return {
            "task_type": "ä¿¡æ¯æŸ¥è¯¢",
            "steps": ["åˆ†æé—®é¢˜", "ç”Ÿæˆå›ç­”"],
            "required_tools": [],
            "need_knowledge_base": False
        }


def format_tools_description(tool_records: List[ToolRecord]) -> str:
    """æ ¼å¼åŒ–å·¥å…·æè¿°ä¾› LLM ç†è§£"""
    if not tool_records:
        return "æ— å¯ç”¨å·¥å…·"
    
    descriptions = []
    for tool in tool_records:
        try:
            config = json.loads(tool.config or "{}")
            builtin_key = config.get("builtin_key", "")
            descriptions.append(
                f"- {tool.id}: {tool.name} ({builtin_key}) - {tool.description}"
            )
        except:
            descriptions.append(f"- {tool.id}: {tool.name} - {tool.description}")
    
    return "\n".join(descriptions)


# ==================== çŠ¶æ€å®šä¹‰ ====================
class AgentState(TypedDict):
    """Agent çš„çŠ¶æ€ï¼Œè´¯ç©¿æ•´ä¸ªå·¥ä½œæµ"""
    
    # åŸºç¡€ä¿¡æ¯
    user_query: str  # ç”¨æˆ·åŸå§‹é—®é¢˜
    conversation_history: Annotated[Sequence[Dict[str, str]], operator.add]  # å¯¹è¯å†å²
    session_id: Optional[str]  # ä¼šè¯IDï¼Œç”¨äºé•¿æœŸè®°å¿†
    user_id: Optional[str]  # ç”¨æˆ·IDï¼Œç”¨äºå¤šç”¨æˆ·åœºæ™¯
    difficulty: Optional[str]  # ä»»åŠ¡éš¾åº¦ï¼šsimple, hard
    pre_generated_answer: Optional[str]  # é¢„ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆç”¨äºç®€å•ä»»åŠ¡å¿«é€Ÿå“åº”ï¼‰
    stream_mode: Optional[bool]  # æ˜¯å¦ä¸ºæµå¼æ¨¡å¼
    
    # è§„åˆ’ä¿¡æ¯
    plan: Optional[str]  # Agent ç”Ÿæˆçš„è®¡åˆ’
    current_step: int  # å½“å‰æ‰§è¡Œåˆ°ç¬¬å‡ æ­¥
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    # å·¥å…·ç›¸å…³
    available_tools: List[str]  # å¯ç”¨çš„å·¥å…·IDåˆ—è¡¨
    tool_calls_made: Annotated[List[Dict[str, Any]], operator.add]  # å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨
    tool_results: Annotated[List[Dict[str, Any]], operator.add]  # å·¥å…·æ‰§è¡Œç»“æœ
    skipped_tasks: Annotated[List[Dict[str, Any]], operator.add]  # è¢«è·³è¿‡çš„ä»»åŠ¡åŠåŸå› 
    
    # RAG ç›¸å…³
    use_knowledge_base: bool  # æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“
    retrieved_contexts: List[Dict[str, Any]]  # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    
    # Agent æ€è€ƒè¿‡ç¨‹
    thoughts: Annotated[List[str], operator.add]  # Agent çš„æ€è€ƒè¿‡ç¨‹
    observations: Annotated[List[str], operator.add]  # è§‚å¯Ÿåˆ°çš„ç»“æœ
    
    # éšå¼æ¨ç†
    subquestions: List[str]
    answer_outline: List[str]
    evidence_requirements: List[str]
    reasoning_steps: Annotated[List[str], operator.add]
    react_cursor: int
    react_max_steps: int
    react_steps_done: int
    
    # å†³ç­–ç›¸å…³
    next_action: Optional[str]  # ä¸‹ä¸€æ­¥åŠ¨ä½œï¼štool_call, search_kb, synthesize, complete
    needs_human_input: bool  # æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
    human_feedback: Optional[str]  # äººå·¥åé¦ˆ
    
    # è´¨é‡æ§åˆ¶
    reflection: Optional[str]  # åæ€ç»“æœ
    quality_score: float  # è´¨é‡è¯„åˆ† 0-1
    
    # æœ€ç»ˆè¾“å‡º
    final_answer: Optional[str]  # æœ€ç»ˆç­”æ¡ˆ
    final_prompt: Optional[str]  # æœ€ç»ˆç”Ÿæˆçš„ Promptï¼ˆç”¨äºæµå¼è¾“å‡ºï¼‰
    ready_to_synthesize: Optional[bool]  # æ˜¯å¦å‡†å¤‡å¥½åˆæˆï¼ˆç”¨äºå»¶è¿Ÿåˆæˆï¼‰
    is_complete: bool  # æ˜¯å¦å®Œæˆ
    error: Optional[str]  # é”™è¯¯ä¿¡æ¯


# ==================== æ ¸å¿ƒèŠ‚ç‚¹å‡½æ•° ====================

async def planner_node(
    state: AgentState,
    settings: Settings,
    tool_records: List[ToolRecord],
    session: Session = None,
    session_id: str = None,
    user_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    è§„åˆ’å™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM åˆ†æç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆæ™ºèƒ½æ‰§è¡Œè®¡åˆ’
    """
    logger.info("ğŸ§  [è§„åˆ’å™¨] å¼€å§‹æ™ºèƒ½åˆ†æä»»åŠ¡...")
    
    user_query = state["user_query"]
    use_knowledge_base = state.get("use_knowledge_base", False)
    
    # æ£€ç´¢ç›¸å…³è®°å¿†
    relevant_memories = []
    if session and (session_id or user_id):
        try:
            relevant_memories = await retrieve_relevant_memories(
                session=session,
                query=user_query,
                settings=settings,
                user_id=user_id,
                session_id=session_id,
                max_memories=5,
            )
            if relevant_memories:
                logger.info(f"ğŸ“š åœ¨è§„åˆ’å™¨ä¸­æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
        except Exception as e:
            logger.warning(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
    
    # æ ¼å¼åŒ–å·¥å…·æè¿°
    tools_desc = format_tools_description(tool_records)
    
    # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
    memory_context = ""
    if relevant_memories:
        memory_lines = [f"- {mem.content}" for mem in relevant_memories]
        memory_context = f"\nç”¨æˆ·å·²çŸ¥ä¿¡æ¯ï¼ˆç”¨äºè§„åˆ’å‚è€ƒï¼‰ï¼š\n" + "\n".join(memory_lines) + "\n"
    
    # æ„å»ºæ™ºèƒ½è§„åˆ’æç¤ºè¯ï¼ˆé¿å… f-string ä¸­çš„èŠ±æ‹¬å·å¯¼è‡´æ ¼å¼é”™è¯¯ï¼‰
    header = (
        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·é—®é¢˜ï¼Œåˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼Œå¹¶ç»™å‡ºéšå¼æ¨ç†è‰ç¨¿ã€‚\n\n"
        f"ç”¨æˆ·é—®é¢˜ï¼š{user_query}\n"
        f"{memory_context}"
        "å¯ç”¨å·¥å…·ï¼š\n"
        f"{tools_desc}\n\n"
        f"çŸ¥è¯†åº“ï¼š{'å·²å¯ç”¨' if use_knowledge_base else 'æœªå¯ç”¨'}\n\n"
        "è¯·åˆ†æä»»åŠ¡å¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºè®¡åˆ’ã€‚\n\n"
        "**å†³ç­–é€»è¾‘**ï¼š\n"
        "1. **ç›´æ¥å›ç­” (Direct Answer)**ï¼šå¦‚æœé—®é¢˜æ˜¯å¸¸è¯†ã€æ¦‚å¿µè§£é‡Šã€é—²èŠï¼Œä¸”ä½ æ— éœ€ä½¿ç”¨å·¥å…·æˆ–æœç´¢å³å¯å›ç­”ï¼Œè¯·é€‰æ‹©æ­¤æ¨¡å¼ã€‚\n"
        "2. **å·¥å…·è°ƒç”¨ (Tool Use)**ï¼šå¦‚æœéœ€è¦æœç´¢ã€ç”»å›¾ã€æŸ¥å¤©æ°”ã€åšç¬”è®°ç­‰ï¼Œè¯·é€‰æ‹©æ­¤æ¨¡å¼ã€‚\n"
        "3. **çŸ¥è¯†åº“æ£€ç´¢ (RAG)**ï¼šå¦‚æœéœ€è¦ä»çŸ¥è¯†åº“æŸ¥æ‰¾ä¿¡æ¯ï¼ˆä¸”çŸ¥è¯†åº“å·²å¯ç”¨ï¼‰ï¼Œè¯·é€‰æ‹©æ­¤æ¨¡å¼ã€‚\n\n"
    )
    json_template = """
è¯·è¿”å› JSONï¼š
{
  "task_type": "direct_answer|tool_use|rag_search|complex_task",
  "analysis": "ä»»åŠ¡åˆ†æç®€è¿°",
  "steps": ["æ­¥éª¤1", "æ­¥éª¤2", "..."],
  "required_tools": ["tool_id_1", "..."],
  "direct_answer_content": "å¦‚æœæ˜¯direct_answeræ¨¡å¼ï¼Œè¯·åœ¨æ­¤ç›´æ¥å†™å‡ºå®Œæ•´å›ç­”ï¼ˆæ”¯æŒMarkdownï¼‰ï¼›å¦åˆ™ç•™ç©º",
  "need_knowledge_base": true,
  "subquestions": ["å­é—®é¢˜1", "å­é—®é¢˜2", "..."],
  "answer_outline": ["ç« èŠ‚1", "ç« èŠ‚2", "..."],
  "evidence_requirements": ["å¿…é¡»è¦†ç›–çš„è¦ç‚¹æˆ–è¯æ®1", "è¦ç‚¹2", "..."]
}

æ³¨æ„ï¼š
1. ä¼˜å…ˆå°è¯• **direct_answer** ä»¥æä¾›æœ€å¿«å“åº”ã€‚
2. åªæœ‰å½“ç¡®å®éœ€è¦å¤–éƒ¨ä¿¡æ¯æ—¶æ‰ä½¿ç”¨ tool_use æˆ– rag_searchã€‚
3. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
    planning_prompt = header + json_template
    
    try:
        # è°ƒç”¨ LLM è¿›è¡Œè§„åˆ’
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": planning_prompt}],
            settings=settings,
            temperature=0.3,  # ä½æ¸©åº¦ä¿è¯è§„åˆ’ç¨³å®š
            max_tokens=1500
        )
        
        # è§£æ LLM è¿”å›çš„ JSON
        plan_data = parse_json_from_llm(llm_response)
        
        task_type = plan_data.get("task_type", "complex_task")
        analysis = plan_data.get("analysis", "åˆ†æä»»åŠ¡ä¸­...")
        
        # === Level 2: Direct Reasoning (ç›´æ¥æ¨ç†) ===
        if task_type == "direct_answer" and plan_data.get("direct_answer_content"):
            logger.info("ğŸš€ [è§„åˆ’å™¨] åˆ¤å®šä¸ºç›´æ¥å›ç­”æ¨¡å¼ (Level 2)")
            return {
                "plan": "ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜",
                "current_step": 0,
                "thoughts": ["Planner: åˆ¤å®šä¸ºé€šç”¨çŸ¥è¯†/é—²èŠï¼Œç›´æ¥ç”Ÿæˆå›ç­”"],
                "next_action": "synthesize",
                "difficulty": "simple",
                "pre_generated_answer": plan_data.get("direct_answer_content")
            }

        steps = plan_data.get("steps", ["åˆ†æé—®é¢˜", "ç”Ÿæˆç­”æ¡ˆ"])
        
        logger.info(f"ğŸ“‹ è§„åˆ’å®Œæˆï¼š{task_type}, {len(steps)} ä¸ªæ­¥éª¤")
        
        # æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬
        plan_text = f"""ä»»åŠ¡ç±»å‹ï¼š{task_type}
ä»»åŠ¡åˆ†æï¼š{analysis}

æ‰§è¡Œæ­¥éª¤ï¼š
{chr(10).join(f"{i+1}. {step}" for i, step in enumerate(steps))}
"""
        
        thought = f"æ™ºèƒ½è§„åˆ’å®Œæˆï¼šè¯†åˆ«ä¸ºã€{task_type}ã€‘ï¼Œå…± {len(steps)} ä¸ªæ­¥éª¤"
        
        return {
            "plan": plan_text,
            "current_step": 0,
            "thoughts": [thought],
            "next_action": "route",
            "subquestions": plan_data.get("subquestions", []),
            "answer_outline": plan_data.get("answer_outline", []),
            "evidence_requirements": plan_data.get("evidence_requirements", [])
        }
    
    except Exception as e:
        logger.error(f"è§„åˆ’å™¨å¤±è´¥: {e}")
        # é™çº§åˆ°ç®€å•è§„åˆ’
        fallback_plan = f"""ä»»åŠ¡åˆ†æï¼šç”¨æˆ·è¯¢é—®ã€Œ{user_query}ã€

æ‰§è¡Œæ­¥éª¤ï¼š
1. æ ¹æ®é—®é¢˜é€‰æ‹©åˆé€‚çš„å¤„ç†æ–¹å¼
2. æ”¶é›†å¿…è¦çš„ä¿¡æ¯
3. ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ

é¢„æœŸç»“æœï¼šä¸ºç”¨æˆ·æä¾›æœ‰ç”¨çš„å›ç­”
"""
        return {
            "plan": fallback_plan,
            "current_step": 0,
            "thoughts": [f"ä½¿ç”¨ç®€åŒ–è§„åˆ’æ¨¡å¼ï¼ˆè§„åˆ’å™¨å¼‚å¸¸ï¼š{str(e)[:50]}ï¼‰"],
            "next_action": "route"
        }


async def router_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    è·¯ç”±å™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM æ™ºèƒ½å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œ
    """
    logger.info("ğŸ”€ [è·¯ç”±å™¨] æ™ºèƒ½å†³ç­–ä¸‹ä¸€æ­¥åŠ¨ä½œ...")
    
    user_query = state["user_query"]
    current_step = state.get("current_step", 0)
    max_iterations = state.get("max_iterations", 10)
    tool_calls_made = state.get("tool_calls_made", [])
    use_knowledge_base = state.get("use_knowledge_base", False)
    observations = state.get("observations", [])
    retrieved_contexts = state.get("retrieved_contexts", [])
    tool_results = state.get("tool_results", [])
    
    # === å¿«é€Ÿé€šé“æ£€æŸ¥ ===
    # å¦‚æœ Planner å·²ç»å†³å®šäº†ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆä¾‹å¦‚ simple ä»»åŠ¡ï¼‰ï¼Œç›´æ¥æ‰§è¡Œ
    pre_decided_action = state.get("next_action")
    if pre_decided_action == "synthesize" and state.get("difficulty") == "simple":
        logger.info("ğŸš€ [è·¯ç”±å™¨] æ£€æµ‹åˆ°å¿«é€Ÿé€šé“åŠ¨ä½œï¼Œè·³è¿‡å†³ç­–")
        return {
            "next_action": "synthesize",
            "thoughts": ["å¿«é€Ÿé€šé“ï¼šç›´æ¥è¿›å…¥åˆæˆé˜¶æ®µ"],
            "current_step": current_step + 1
        }
    
    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
    if current_step >= max_iterations:
        return {
            "next_action": "synthesize",
            "thoughts": [f"å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"],
            "current_step": current_step + 1
        }
    
    # æ£€æµ‹çŸ¥è¯†åº“æ˜¯å¦å·²ç»æœç´¢è¿‡
    kb_searched = any("çŸ¥è¯†åº“" in obs for obs in observations)
    kb_empty = kb_searched and not retrieved_contexts  # æœç´¢è¿‡ä½†æ²¡æœ‰ç»“æœ
    
    # å¦‚æœç¬¬ä¸€æ­¥ï¼Œå…ˆè¿›è¡Œç®€å•åˆ¤æ–­ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
    if current_step == 0:
        # å¯ç”¨çŸ¥è¯†åº“ä½†æœªæ£€ç´¢
        if use_knowledge_base and not kb_searched:
            return {
                "next_action": "search_kb",
                "thoughts": ["é¦–æ¬¡æ‰§è¡Œï¼šä¼˜å…ˆæ£€ç´¢çŸ¥è¯†åº“"],
                "current_step": current_step + 1
            }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·
        if should_call_tool(state):
            return {
                "next_action": "tool_executor",
                "thoughts": ["é¦–æ¬¡æ‰§è¡Œï¼šæ£€æµ‹åˆ°éœ€è¦å·¥å…·è°ƒç”¨"],
                "current_step": current_step + 1
            }
    
    # å¦‚æœçŸ¥è¯†åº“å·²æœç´¢ä½†ä¸ºç©ºï¼Œä¸”æ²¡æœ‰éœ€è¦å·¥å…·çš„ä»»åŠ¡ï¼Œç›´æ¥è¿›å…¥åˆæˆé˜¶æ®µ
    if current_step >= 1 and kb_empty and not should_call_tool(state):
        logger.info("âš ï¸ çŸ¥è¯†åº“å·²æœç´¢ä½†ä¸ºç©ºï¼Œä¸”æ— éœ€å·¥å…·ï¼Œç›´æ¥è¿›å…¥åˆæˆé˜¶æ®µ")
        return {
            "next_action": "synthesize",
            "thoughts": ["çŸ¥è¯†åº“ä¸ºç©ºä¸”æ— éœ€å·¥å…·ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆ"],
            "current_step": current_step + 1
        }
    
    # æ­¥éª¤ >= 1ï¼Œä½¿ç”¨ LLM æ™ºèƒ½å†³ç­–ï¼ˆReAct æ§åˆ¶å™¨å‰ç½®ï¼‰
    # å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œé¿å…é‡å¤æœç´¢
    kb_already_searched = len(retrieved_contexts) > 0 or any("çŸ¥è¯†åº“" in obs or "æ£€ç´¢åˆ°" in obs for obs in observations)
    
    try:
        # æ„å»ºå†³ç­–ä¸Šä¸‹æ–‡
        kb_status = "å·²æ£€ç´¢" if kb_already_searched else "æœªæ£€ç´¢"
        kb_status_detail = f"å·²æ£€ç´¢ {len(retrieved_contexts)} æ¡" if retrieved_contexts else "æœªæ£€ç´¢"
        
        context_summary = f"""å½“å‰æ‰§è¡ŒçŠ¶æ€ï¼š
- ç”¨æˆ·é—®é¢˜ï¼š{user_query}
- æ‰§è¡Œæ­¥éª¤ï¼š{current_step}/{max_iterations}
- å·²è°ƒç”¨å·¥å…·æ•°ï¼š{len(tool_calls_made)}
- çŸ¥è¯†åº“æ£€ç´¢ï¼š{"å·²æ£€ç´¢ä½†æ— ç»“æœ" if kb_empty else ("å·²æ£€ç´¢ " + str(len(retrieved_contexts)) + " æ¡" if retrieved_contexts else "æœªæ£€ç´¢")}
- å·¥å…·æ‰§è¡Œç»“æœæ•°ï¼š{len(tool_results)}

æœ€è¿‘è§‚å¯Ÿï¼š
{chr(10).join("- " + obs for obs in observations[-3:]) if observations else "æš‚æ— è§‚å¯Ÿ"}

è¯·åˆ¤æ–­ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼š
A. search_kb - éœ€è¦ä»çŸ¥è¯†åº“æ£€ç´¢ä¿¡æ¯
B. tool_executor - éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·è·å–æ•°æ®
C. synthesize - ä¿¡æ¯å·²è¶³å¤Ÿï¼Œå¯ä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

è¦æ±‚ï¼š
1. **é‡è¦**ï¼šå¦‚æœçŸ¥è¯†åº“å·²ç»æœç´¢è¿‡ä½†æ— ç»“æœï¼ˆå·²æ£€ç´¢ä½†æ— ç»“æœï¼‰ï¼Œä¸è¦é€‰æ‹© Aï¼Œåº”è¯¥é€‰æ‹© C
2. å¦‚æœå¯ç”¨äº†çŸ¥è¯†åº“ä½†è¿˜æ²¡æ£€ç´¢ï¼ˆçŸ¥è¯†åº“æ£€ç´¢æ˜¾ç¤º"æœªæ£€ç´¢"ï¼‰ï¼Œä¼˜å…ˆé€‰æ‹© A
3. å¦‚æœçŸ¥è¯†åº“å·²ç»æ£€ç´¢è¿‡ï¼ˆçŸ¥è¯†åº“æ£€ç´¢æ˜¾ç¤º"å·²æ£€ç´¢"ï¼‰ï¼Œä¸è¦é‡å¤é€‰æ‹© Aï¼Œåº”è¯¥é€‰æ‹© B æˆ– C
4. å¦‚æœé—®é¢˜éœ€è¦å¤šä¸ªå·¥å…·ï¼ˆå¦‚ï¼šæœç´¢+ç»˜å›¾ï¼‰ï¼Œå¿…é¡»æ‰§è¡Œå®Œæ‰€æœ‰å·¥å…·åå†é€‰æ‹© C
5. å¦‚æœé—®é¢˜éœ€è¦å®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€æœç´¢ç­‰ï¼‰ï¼Œä½†è¿˜æ²¡è°ƒç”¨ç›¸åº”å·¥å…·ï¼Œé€‰æ‹© B
6. å¦‚æœå·²æœ‰è¶³å¤Ÿä¿¡æ¯ä¸”æ‰€æœ‰å¿…è¦å·¥å…·éƒ½å·²æ‰§è¡Œï¼Œé€‰æ‹© C
7. åªå›å¤ä¸€ä¸ªå­—æ¯ï¼ˆA/B/Cï¼‰ï¼Œä¸è¦è§£é‡Š
"""
        
        # è°ƒç”¨ LLM å†³ç­–
        llm_response, _ = await invoke_llm(
            messages=[{"role": "user", "content": context_summary}],
            settings=settings,
            temperature=0.1,  # æä½æ¸©åº¦ä¿è¯å†³ç­–ä¸€è‡´æ€§
            max_tokens=10
        )
        
        decision = llm_response.strip().upper()
        
        # æ˜ å°„å†³ç­–
        action_map = {
            "A": "search_kb",
            "B": "tool_executor",
            "C": "synthesize"
        }
        
        next_action = action_map.get(decision, "synthesize")
        
        # å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœçŸ¥è¯†åº“å·²æœç´¢ä½†ä¸ºç©ºï¼Œä¸”é€‰æ‹©äº† search_kbï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize
        if kb_empty and next_action == "search_kb":
            logger.warning("âš ï¸ çŸ¥è¯†åº“å·²ä¸ºç©ºï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize")
            next_action = "synthesize"
            decision = "C"
        
        # é˜²æ­¢é‡å¤æœç´¢çŸ¥è¯†åº“ï¼šå¦‚æœå·²ç»æ£€ç´¢è¿‡ï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize æˆ– tool_executor
        if next_action == "search_kb" and kb_already_searched:
            logger.warning(f"âš ï¸ é˜»æ­¢é‡å¤çŸ¥è¯†åº“æœç´¢ï¼šå·²æ£€ç´¢è¿‡ {len(retrieved_contexts)} æ¡ï¼Œå¼ºåˆ¶æ”¹ä¸º synthesize")
            if should_call_tool(state):
                next_action = "tool_executor"
                thought = "LLMé€‰æ‹©Aä½†å·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œæ”¹ä¸ºè°ƒç”¨å·¥å…·"
            else:
                next_action = "synthesize"
                thought = "LLMé€‰æ‹©Aä½†å·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œæ”¹ä¸ºç”Ÿæˆç­”æ¡ˆ"
        else:
            thought = f"LLM æ™ºèƒ½è·¯ç”±ï¼š{decision} -> {next_action}"
        
        logger.info(f"ğŸ“ æ™ºèƒ½è·¯ç”±å†³ç­–ï¼šæ­¥éª¤{current_step}, å†³ç­–={decision}, ä¸‹ä¸€æ­¥={next_action}")
        
        return {
            "next_action": next_action,
            "thoughts": [thought],
            "current_step": current_step + 1
        }
    
    except Exception as e:
        logger.error(f"è·¯ç”±å™¨ LLM å†³ç­–å¤±è´¥: {e}")
        
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨ç®€å•è§„åˆ™
        kb_searched = len(retrieved_contexts) > 0 or any("çŸ¥è¯†åº“" in obs or "æ£€ç´¢åˆ°" in obs for obs in observations)
        
        if kb_empty and not should_call_tool(state):
            next_action = "synthesize"
            thought = "é™çº§å†³ç­–ï¼šçŸ¥è¯†åº“ä¸ºç©ºï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆ"
        elif use_knowledge_base and not kb_searched and current_step < 2:
            # é˜²æ­¢é‡å¤æœç´¢ï¼šå¦‚æœå·²ç»æ£€ç´¢è¿‡ï¼Œä¸å†é€‰æ‹© search_kb
            next_action = "search_kb"
            thought = "é™çº§å†³ç­–ï¼šæ£€ç´¢çŸ¥è¯†åº“"
        elif kb_searched and current_step >= 2:
            # å·²ç»æ£€ç´¢è¿‡ï¼Œå¦‚æœè¿˜æœ‰å·¥å…·è¦è°ƒç”¨å°±è°ƒç”¨å·¥å…·ï¼Œå¦åˆ™ç”Ÿæˆç­”æ¡ˆ
            if should_call_tool(state):
                next_action = "tool_executor"
                thought = "é™çº§å†³ç­–ï¼šå·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œè°ƒç”¨å·¥å…·"
            else:
                next_action = "synthesize"
                thought = "é™çº§å†³ç­–ï¼šå·²æ£€ç´¢è¿‡çŸ¥è¯†åº“ï¼Œç”Ÿæˆç­”æ¡ˆ"
        elif should_call_tool(state):
            next_action = "tool_executor"
            thought = "é™çº§å†³ç­–ï¼šè°ƒç”¨å·¥å…·"
        else:
            next_action = "synthesize"
            thought = "é™çº§å†³ç­–ï¼šç”Ÿæˆç­”æ¡ˆ"
        
        return {
            "next_action": next_action,
            "thoughts": [thought],
            "current_step": current_step + 1
        }

async def react_controller_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    subqs = state.get("subquestions", []) or []
    evids = state.get("evidence_requirements", []) or []
    cursor = state.get("react_cursor", 0)
    max_steps = state.get("react_max_steps", 4)
    steps_done = state.get("react_steps_done", 0)
    retrieved_contexts = state.get("retrieved_contexts", []) or []
    tool_results = state.get("tool_results", []) or []
    coverage = (len(retrieved_contexts) + len(tool_results)) / max(1, len(subqs) or 1)
    if steps_done >= max_steps or coverage >= 0.65:
        return {
            "next_action": "synthesize",
            "thoughts": [f"ReActç»“æŸï¼šsteps={steps_done}, coverage={coverage:.2f}"],
        }
    current_subq = subqs[cursor] if cursor < len(subqs) else ""
    prompt = f"""ä½ æ˜¯ä»»åŠ¡æ§åˆ¶å™¨ã€‚æ ¹æ®å½“å‰å­é—®é¢˜å’Œè§‚å¯Ÿï¼Œå†³å®šä¸‹ä¸€æ­¥å·¥å…·è°ƒç”¨æˆ–åœæ­¢ã€‚
å­é—®é¢˜ï¼š{current_subq}
è¯æ®éœ€æ±‚ï¼š{'; '.join(evids[:5])}
æœ€è¿‘è§‚å¯Ÿï¼š{'; '.join(state.get('observations', [])[-3:])}
å¯é€‰åŠ¨ä½œï¼š
A.web_search
B.knowledge_search
C.draw_diagram
D.stop
åªè¾“å‡ºä¸€ä¸ªå­—æ¯ã€‚"""
    try:
        reply, _ = await invoke_llm(
            messages=[{"role": "user", "content": prompt}],
            settings=settings,
            temperature=0.0,
            max_tokens=4,
        )
        action = reply.strip().upper()[:1]
        map_act = {"A": "tool_executor", "B": "knowledge_search", "C": "tool_executor", "D": "synthesize"}
        next_action = map_act.get(action, "tool_executor")
        next_cursor = cursor + (1 if next_action in ("synthesize",) else 0)
        return {
            "next_action": next_action,
            "react_cursor": next_cursor,
            "react_steps_done": steps_done + 1,
            "thoughts": [f"ReActå†³ç­–ï¼š{action} -> {next_action} | cursor={next_cursor}"],
        }
    except Exception as e:
        return {
            "next_action": "tool_executor",
            "react_steps_done": steps_done + 1,
            "thoughts": [f"ReActé™çº§ï¼šå¼‚å¸¸ {str(e)[:60]}ï¼Œæ”¹ä¸ºå·¥å…·æ‰§è¡Œ"],
        }

def knowledge_search_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    """
    çŸ¥è¯†åº“æœç´¢èŠ‚ç‚¹ï¼šä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³å†…å®¹
    """
    logger.info("ğŸ“š [çŸ¥è¯†åº“] æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    
    user_query = state["user_query"]
    
    try:
        # è°ƒç”¨ RAG æ£€ç´¢
        contexts = retrieve_context(
            query=user_query,
            settings=settings,
            top_k=4
        )
        
        retrieved = [
            {
                "document_id": ctx.document_id,
                "original_name": ctx.original_name,
                "content": ctx.content[:500]  # é™åˆ¶é•¿åº¦
            }
            for ctx in contexts
        ]
        
        observation = f"ä»çŸ¥è¯†åº“æ£€ç´¢åˆ° {len(retrieved)} ä¸ªç›¸å…³ç‰‡æ®µ"
        
        return {
            "retrieved_contexts": retrieved,
            "observations": [observation],
            "thoughts": ["çŸ¥è¯†åº“æ£€ç´¢å®Œæˆï¼Œè·å–åˆ°ç›¸å…³èƒŒæ™¯ä¿¡æ¯"]
        }
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
        return {
            "retrieved_contexts": [],
            "observations": [f"çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {str(e)}"],
            "error": str(e)
        }


async def tool_executor_node(
    state: AgentState,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
) -> Dict[str, Any]:
    """å·¥å…·æ‰§è¡Œå™¨èŠ‚ç‚¹ï¼šæ™ºèƒ½é€‰æ‹©å¹¶æ‰§è¡Œå·¥å…·ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰"""
    logger.info("ğŸ”§ [å·¥å…·æ‰§è¡Œå™¨] å‡†å¤‡è°ƒç”¨å·¥å…·...")

    user_query = state.get("user_query", "")
    tool_calls_made = state.get("tool_calls_made", [])
    tool_results = state.get("tool_results", [])
    skipped_tasks = state.get("skipped_tasks", [])

    tasks = infer_tool_tasks(user_query)
    if not tasks:
        observation = f"åˆ†ææŸ¥è¯¢æœªå‘ç°éœ€è¦è°ƒç”¨å·¥å…·çš„æŒ‡ä»¤ï¼š{user_query}" if user_query else "æ— éœ€è°ƒç”¨å·¥å…·"
        return {
            "thoughts": ["æœªæ‰¾åˆ°éœ€è¦æ‰§è¡Œçš„å·¥å…·ä»»åŠ¡"],
            "observations": [observation],
            "next_action": "synthesize",
        }

    completed_tasks = {call.get("task") for call in tool_calls_made if call.get("task")}
    skipped_task_keys = {
        item.get("task")
        for item in skipped_tasks
        if isinstance(item, dict) and item.get("task")
    }

    tool_index: Dict[str, ToolRecord] = {}
    for record in tool_records:
        if getattr(record, "is_active", True):
            task_key = map_tool_to_task(record)
            if task_key and task_key not in tool_index:
                tool_index[task_key] = record

    # 1. è¯†åˆ«æ‰€æœ‰å¯å¹¶è¡Œçš„å¾…å¤„ç†ä»»åŠ¡
    tasks_to_run = []
    
    for task in tasks:
        if task in completed_tasks or task in skipped_task_keys:
            continue
        
        # ä¾èµ–æ£€æŸ¥é€»è¾‘
        # Diagram éœ€è¦ Search ç»“æœ
        if task == "diagram" and "search" in tasks and "search" not in completed_tasks:
            continue
        # Note éœ€è¦ Weather ç»“æœ
        if task == "note" and "weather" in tasks and "weather" not in completed_tasks:
            continue
            
        tool = tool_index.get(task)
        if not tool:
            reason = f"æ‰¾ä¸åˆ°ä»»åŠ¡ {task} å¯¹åº”çš„å·¥å…·"
            logger.warning(reason)
            # è®°å½•è·³è¿‡ï¼Œä½†æš‚ä¸è¿”å›ï¼Œç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡
            # (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæœ¬æ¬¡å¾ªç¯ä¸è·‘å®ƒï¼Œä¸‹æ¬¡å¾ªç¯ä¼šå†æ¬¡æ£€æµ‹å¹¶å¯èƒ½æ ‡è®°è·³è¿‡)
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªæ·»åŠ æœ‰æ•ˆçš„
            continue
            
        # å‡†å¤‡å‚æ•°
        tool_args = {}
        action_description = ""
        
        # å‚æ•°æ„é€ é€»è¾‘ (ä¿æŒåŸæœ‰é€»è¾‘)
        if task == "weather":
            city = extract_city_from_query(user_query)
            tool_args = {"city": city}
            action_description = f"æŸ¥è¯¢{city}å¤©æ°”"
        elif task == "search":
            search_query = extract_search_query(user_query)
            tool_args = {"query": search_query, "num_results": 6}
            action_description = f"æœç´¢'{search_query}'è·å–ä¿¡æ¯"
        elif task == "diagram":
            # æ­¤æ—¶ search åº”è¯¥å·²å®Œæˆ (ä¾èµ–æ£€æŸ¥è¿‡äº†)
            search_context = None
            for result in reversed(tool_results):
                if result.get("task") == "search":
                    search_context = result.get("output", "")[:2000]
                    break
            
            if search_context:
                try:
                    payload = await generate_diagram_payload_with_llm(user_query, search_context, settings)
                    tool_args = payload
                    action_description = "åŸºäºæœç´¢ç»“æœä½¿ç”¨LLMç”Ÿæˆæ€ç»´å¯¼å›¾"
                except Exception as e:
                    logger.warning(f"LLMç”Ÿæˆæ€ç»´å¯¼å›¾å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–¹æ³•: {e}")
                    payload = generate_diagram_payload(user_query, search_context)
                    tool_args = payload
                    action_description = "åŸºäºæœç´¢ç»“æœç”Ÿæˆæ€ç»´å¯¼å›¾"
            else:
                payload = generate_diagram_payload(user_query, None)
                tool_args = payload
                action_description = "ç”Ÿæˆæ€ç»´å¯¼å›¾"
        elif task == "note":
            # æ­¤æ—¶ weather åº”è¯¥å·²å®Œæˆ
            weather_result = None
            for result in reversed(tool_results):
                if result.get("task") == "weather" or "å¤©æ°”" in result.get("tool_name", ""):
                    weather_result = result
                    break
            
            # åœºæ™¯1ï¼šå¸¦ä¼æé†’
            if weather_result and any(kw in user_query for kw in ["å¸¦ä¼", "é›¨ä¼", "æé†’"]):
                weather_text = weather_result.get("output", "")
                if not detect_rain_in_text(weather_text):
                    # æ— é›¨ï¼Œè·³è¿‡
                    continue
                
                city_from_weather = weather_result.get("arguments", {}).get("city")
                if not city_from_weather:
                    city_from_weather = extract_city_from_query(user_query)
                filename = build_note_filename(city_from_weather)
                note_content = build_note_content(city_from_weather, weather_text, user_query)
                tool_args = {"filename": filename, "content": note_content}
                action_description = f"ä¸º{city_from_weather}åˆ›å»ºå¸¦ä¼æé†’"
            else:
                # åœºæ™¯2ï¼šé€šç”¨ç¬”è®°
                # ... (åŸæœ‰é€»è¾‘)
                context_parts = []
                if tool_results:
                    for tr in tool_results:
                        tool_name = tr.get("tool_name", "å·¥å…·")
                        output = tr.get("output", "")
                        context_parts.append(f"ã€{tool_name}ç»“æœã€‘\n{output[:800]}")
                context_text = "\n\n".join(context_parts) if context_parts else "æ— å·¥å…·ç»“æœ"
                
                # è¿™é‡Œç®€åŒ–ï¼šä¸å†å®æ—¶è°ƒç”¨LLMç”Ÿæˆï¼Œé¿å…é˜»å¡å¹¶å‘ã€‚æˆ–è€…ä¹Ÿæ”¾å…¥ thread pool?
                # ä¸ºäº†ä¿æŒç®€å•ï¼Œå‡è®¾ note å†…å®¹æ„å»ºä¸ä¾èµ–å¤æ‚ LLM äº¤äº’ï¼Œæˆ–è€…å…è®¸åœ¨è¿™é‡Œè°ƒç”¨
                # åŸæœ‰é€»è¾‘ç”¨äº†ç®€å•çš„ f-stringï¼Œä½† build_note_content æ˜¯ç®€å•çš„ã€‚
                # åªæœ‰ "åœºæ™¯2" ç”¨äº† LLM ç”Ÿæˆç¬”è®°å†…å®¹?
                # åŸä»£ç  line 812 çœ‹èµ·æ¥æ˜¯æ„é€  prompt ä½†æ²¡çœ‹åˆ°è°ƒç”¨?
                # ä»”ç»†çœ‹åŸä»£ç ... å•Šï¼ŒåŸä»£ç  line 812 ä¸‹é¢æ–­æ‰äº†ï¼Œæˆ‘æ²¡è¯»å®Œã€‚
                # å‡è®¾ note é€»è¾‘æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬å…ˆæŠŠå®ƒå½“ä½œæ™®é€šä»»åŠ¡
                # æš‚æ—¶åªæ”¯æŒç®€å•çš„ç¬”è®°ç”Ÿæˆï¼Œæˆ–è€…æŠŠ LLM ç”Ÿæˆé€»è¾‘ç§»åˆ° execute_tool å†…éƒ¨?
                # æš‚ä¸”è·³è¿‡ LLM ç”Ÿæˆç¬”è®°çš„å¤æ‚é€»è¾‘ï¼Œä½¿ç”¨ç®€å•æ¨¡æ¿
                filename = f"note_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                tool_args = {"filename": filename, "content": f"ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}\n\nç›¸å…³ä¿¡æ¯ï¼š\n{context_text}"}
                action_description = "åˆ›å»ºé€šç”¨ç¬”è®°"

        tasks_to_run.append({
            "task": task,
            "tool": tool,
            "args": tool_args,
            "desc": action_description
        })

    if not tasks_to_run:
        # æ²¡æœ‰å¯è¿è¡Œçš„ä»»åŠ¡ï¼ˆå¯èƒ½éƒ½è¢«è·³è¿‡æˆ–å·²å®Œæˆï¼‰
        return {
            "thoughts": ["å½“å‰æ— å¾…æ‰§è¡Œä»»åŠ¡"],
            "next_action": "synthesize",
        }

    # 2. å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    logger.info(f"ğŸš€ å¹¶è¡Œæ‰§è¡Œ {len(tasks_to_run)} ä¸ªä»»åŠ¡: {[t['task'] for t in tasks_to_run]}")
    
    async def run_one_task(item):
        def _execute_safe(tool, args, settings):
            # åˆ›å»ºæ–°çš„ DB ä¼šè¯ä»¥ä¿è¯çº¿ç¨‹å®‰å…¨
            SessionLocal = get_session_factory()
            with SessionLocal() as db:
                return execute_tool(
                    tool=tool,
                    arguments=args,
                    settings=settings,
                    session=db
                )

        try:
            # ä½¿ç”¨ to_thread åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥å·¥å…·å‡½æ•°
            # æ³¨æ„ï¼šä¸å†ä¼ é€’å¤–éƒ¨çš„ sessionï¼Œè€Œæ˜¯å†…éƒ¨æ–°å»º
            output = await asyncio.to_thread(
                _execute_safe,
                tool=item["tool"],
                args=item["args"],
                settings=settings
            )
            return {
                "task": item["task"],
                "tool_name": item["tool"].name,
                "output": output,
                "arguments": item["args"],
                "success": True,
                "desc": item["desc"]
            }
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {item['task']} æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "task": item["task"],
                "tool_name": item["tool"].name,
                "output": f"æ‰§è¡Œå¤±è´¥: {str(e)}",
                "arguments": item["args"],
                "success": False,
                "desc": item["desc"]
            }

    results = await asyncio.gather(*(run_one_task(item) for item in tasks_to_run))

    # 3. æ±‡æ€»ç»“æœ
    new_tool_calls = []
    new_tool_results = []
    new_thoughts = []
    new_observations = []

    for res in results:
        new_tool_calls.append({"task": res["task"], "tool_id": res["tool_name"], "arguments": res["arguments"]})
        new_tool_results.append(res)
        new_thoughts.append(f"æ‰§è¡Œå·¥å…·: {res['desc']}")
        new_observations.append(f"ã€{res['tool_name']}ã€‘: {res['output'][:200]}...")

    return {
        "tool_calls_made": new_tool_calls,
        "tool_results": new_tool_results,
        "thoughts": new_thoughts,
        "observations": new_observations,
        "next_action": "router",
    }


def reflector_node(state: AgentState) -> Dict[str, Any]:
    """
    åæ€å™¨èŠ‚ç‚¹ï¼šè¯„ä¼°å½“å‰è¿›å±•ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥
    """
    logger.info("ğŸ¤” [åæ€å™¨] è¯„ä¼°å½“å‰è¿›å±•...")
    
    user_query = state["user_query"]
    tool_results = state.get("tool_results", [])
    retrieved_contexts = state.get("retrieved_contexts", [])
    current_step = state.get("current_step", 0)
    
    # è¯„ä¼°ä¿¡æ¯å®Œæ•´æ€§
    has_tool_results = len(tool_results) > 0
    has_kb_context = len(retrieved_contexts) > 0
    
    quality_score = 0.0
    reflection = ""
    
    if has_tool_results or has_kb_context:
        quality_score = 0.7
        reflection = "å·²æ”¶é›†åˆ°ç›¸å…³ä¿¡æ¯ï¼Œå¯ä»¥å°è¯•ç”Ÿæˆç­”æ¡ˆ"
    else:
        quality_score = 0.3
        reflection = "ä¿¡æ¯æ”¶é›†ä¸è¶³ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ£€ç´¢æˆ–å·¥å…·è°ƒç”¨"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
    needs_human = quality_score < 0.5 and current_step > 3
    
    thought = f"åæ€ç»“æœï¼šè´¨é‡è¯„åˆ† {quality_score:.2f}"
    
    return {
        "reflection": reflection,
        "quality_score": quality_score,
        "needs_human_input": needs_human,
        "thoughts": [thought]
    }


def verifier_node(
    state: AgentState,
    settings: Settings,
) -> Dict[str, Any]:
    coverage_ratio = 0.0
    outline = state.get("answer_outline", []) or []
    retrieved_contexts = state.get("retrieved_contexts", []) or []
    tool_results = state.get("tool_results", []) or []
    outline_len = len(outline) if outline else 1
    evidence_count = len(retrieved_contexts) + len(tool_results)
    coverage_ratio = min(1.0, evidence_count / max(1, outline_len))
    need_diagram = any("å¯¼å›¾" in x or "mindmap" in x or "æ€ç»´å¯¼å›¾" in x for x in outline)
    has_diagram = any(r.get("task") == "diagram" for r in tool_results)
    need_search = coverage_ratio < 0.5
    next_action = "synthesizer"
    thought = f"éªŒè¯å™¨ï¼šè¦†ç›–ç‡ {coverage_ratio:.2f}"
    if need_diagram and not has_diagram:
        next_action = "tool_executor"
        thought = f"éªŒè¯å™¨ï¼šéœ€è¦æ€ç»´å¯¼å›¾ï¼Œè§¦å‘å·¥å…·æ‰§è¡Œ"
    elif need_search:
        next_action = "tool_executor"
        thought = f"éªŒè¯å™¨ï¼šè¯æ®ä¸è¶³ï¼Œè§¦å‘å·¥å…·æ‰§è¡Œä»¥è¡¥å……ä¿¡æ¯"
    return {
        "thoughts": [thought],
        "observations": [f"è¦†ç›–ç‡è¯„ä¼°ï¼š{coverage_ratio:.2f}"],
        "next_action": next_action,
    }


async def synthesizer_node(
    state: AgentState,
    settings: Settings,
    session: Session = None,
    session_id: str = None,
    user_id: Optional[str] = None,
) -> Dict[str, Any]:
    """åˆæˆå™¨èŠ‚ç‚¹ï¼šä½¿ç”¨ LLM ç»¼åˆæ‰€æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    logger.info("âœ¨ [åˆæˆå™¨] ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")

    user_query = state.get("user_query", "")
    retrieved_contexts = state.get("retrieved_contexts", [])
    tool_results = state.get("tool_results", [])
    skipped_tasks = state.get("skipped_tasks", [])
    answer_outline = state.get("answer_outline", [])
    subquestions = state.get("subquestions", [])

    # æ£€ç´¢ç›¸å…³è®°å¿†
    relevant_memories = []
    if session and (session_id or user_id):
        try:
            relevant_memories = await retrieve_relevant_memories(
                session=session,
                query=user_query,
                settings=settings,
                user_id=user_id,
                session_id=session_id,
                max_memories=5,
            )
            if relevant_memories:
                logger.info(f"ğŸ“š åœ¨åˆæˆå™¨ä¸­æ£€ç´¢åˆ° {len(relevant_memories)} æ¡ç›¸å…³è®°å¿†")
        except Exception as e:
            logger.warning(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")

    # æ„å»ºä¿¡æ¯ä¸Šä¸‹æ–‡
    context_parts: List[str] = []
    
    # 0. æ·»åŠ è®°å¿†ä¿¡æ¯ï¼ˆæ˜ç¡®æ ‡è¯†ä¸ºç”¨æˆ·å·²çŸ¥ä¿¡æ¯ï¼‰
    memory_context = ""
    if relevant_memories:
        memory_lines = [mem.content for mem in relevant_memories]
        # ç»™è®°å¿†æ·»åŠ æ˜ç¡®æ ‡è¯†ï¼Œç¡®ä¿ LLM èƒ½å¤Ÿè¯†åˆ«å’Œä½¿ç”¨è¿™äº›ä¿¡æ¯
        memory_context = "## ç”¨æˆ·å·²çŸ¥ä¿¡æ¯\n" + "\n".join(f"- {line}" for line in memory_lines)
        context_parts.insert(0, memory_context)
    
    # 1. æ·»åŠ çŸ¥è¯†åº“æ£€ç´¢å†…å®¹
    if retrieved_contexts:
        kb_content = "\n\n".join([
            f"ã€æ–‡æ¡£ç‰‡æ®µ {i+1}ã€‘\næ¥æºï¼š{ctx.get('original_name', 'æœªçŸ¥')}\nå†…å®¹ï¼š{ctx.get('content', '')[:500]}"
            for i, ctx in enumerate(retrieved_contexts[:3])  # æœ€å¤š3ä¸ªç‰‡æ®µ
        ])
        context_parts.append(f"## çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n{kb_content}")
    
    # 2. æ·»åŠ å·¥å…·æ‰§è¡Œç»“æœ
    if tool_results:
        tool_outputs = []
        for tr in tool_results:
            tool_name = tr.get("tool_name", "å·¥å…·")
            output = tr.get("output", "")
            tool_outputs.append(f"ã€{tool_name}ã€‘\n{output[:600]}")
        context_parts.append(f"## å·¥å…·æ‰§è¡Œç»“æœ\n" + "\n\n".join(tool_outputs))
    
    # 3. æ·»åŠ è·³è¿‡çš„ä»»åŠ¡è¯´æ˜
    if skipped_tasks:
        skip_info = "\n".join([
            f"- {item.get('task', 'æœªçŸ¥ä»»åŠ¡')}: {item.get('reason', 'æœªè¯´æ˜')}"
            for item in skipped_tasks
        ])
        context_parts.append(f"## è·³è¿‡çš„ä»»åŠ¡\n{skip_info}")
    
    # åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯ï¼ˆåŒ…æ‹¬è®°å¿†ä¿¡æ¯ï¼‰
    has_info = bool(retrieved_contexts or tool_results or relevant_memories)
    is_simple = state.get("difficulty") == "simple"
    pre_generated_answer = state.get("pre_generated_answer")
    
    try:
        if is_simple and pre_generated_answer:
            logger.info("âš¡ [åˆæˆå™¨] ä½¿ç”¨é¢„ç”Ÿæˆç­”æ¡ˆï¼Œè·³è¿‡ LLM è°ƒç”¨")
            return {
                "final_answer": pre_generated_answer,
                "is_complete": True,
                "thoughts": ["ä½¿ç”¨æ™ºèƒ½åˆ†æé˜¶æ®µç”Ÿæˆçš„ç­”æ¡ˆï¼ŒåŠ é€Ÿå“åº”"],
            }

        if is_simple:
            # === ç®€å•æ¨¡å¼ Prompt ===
            logger.info("âš¡ [åˆæˆå™¨] ä½¿ç”¨å¿«é€Ÿå“åº”æ¨¡å¼")
            all_context = "\n\n".join(context_parts) if context_parts else ""
            
            synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

{all_context}

è¯·ç›´æ¥ã€è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
è¦æ±‚ï¼š
1. è¯­æ°”äº²åˆ‡ï¼Œåƒæœ‹å‹èŠå¤©
2. ç¯‡å¹…ç®€çŸ­é€‚ä¸­ï¼Œä¸è¦é•¿ç¯‡å¤§è®º
3. å¦‚æœæœ‰ç”¨æˆ·è®°å¿†ä¿¡æ¯ï¼Œè¯·è‡ªç„¶åœ°ä½¿ç”¨ï¼ˆå¦‚ç§°å‘¼åå­—ï¼‰
"""
        elif not has_info:
            # æ²¡æœ‰ä»»ä½•é¢å¤–ä¿¡æ¯ï¼Œç›´æ¥è®© LLM åŸºäºè‡ªèº«çŸ¥è¯†å›ç­”
            synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å½“å‰ç³»ç»Ÿæ²¡æœ‰æ£€ç´¢åˆ°çŸ¥è¯†åº“å†…å®¹ï¼Œä¹Ÿæ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ã€‚
è¯·åŸºäºä½ è‡ªèº«çš„çŸ¥è¯†ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å¦‚æœä½ çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯¦ç»†ã€å‡†ç¡®åœ°å›ç­”
2. å¦‚æœä¸ç¡®å®šï¼Œè¯·è¯šå®è¯´æ˜ï¼Œå¹¶ç»™å‡ºå»ºè®®
3. å›ç­”è¦æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
4. ä¸è¦ç¼–é€ ä¿¡æ¯
"""
        else:
            # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
            all_context = "\n\n".join(context_parts) if context_parts else ""
            
            if answer_outline:
                outline_text = "\n".join([f"- {item}" for item in answer_outline[:10]])
                synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

{all_context}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼ŒæŒ‰ç…§ä»¥ä¸‹å¤§çº²åˆ†èŠ‚ç»„ç»‡ç­”æ¡ˆï¼Œæ¯ä¸€èŠ‚ç”¨ç®€æ´å°æ ‡é¢˜ï¼š
{outline_text}

è¦æ±‚ï¼š
1. å›ç­”è¦è‡ªç„¶ã€æµç•…ï¼Œå°±åƒåœ¨å’Œä¸€ä¸ªç†Ÿæ‚‰çš„æœ‹å‹èŠå¤©
2. **é‡è¦**ï¼šå¦‚æœ"ç”¨æˆ·å·²çŸ¥ä¿¡æ¯"ä¸­æœ‰ç”¨æˆ·çš„å§“åã€èŒä¸šç­‰ä¸ªäººä¿¡æ¯ï¼ŒåŠ¡å¿…åœ¨å›ç­”ä¸­è‡ªç„¶åœ°ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·åå«å¼ ä¸‰ï¼Œåœ¨å›ç­”ä¸­å¯ä»¥è¯´"å¼ ä¸‰ï¼Œä½ å¥½"æˆ–"å¼ ä¸‰ï¼Œå…³äºä½ çš„é—®é¢˜..."ï¼‰
3. ä¸è¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ã€ä¿¡æ¯æ¥æºæˆ–æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¦è¯´"æ ¹æ®è®°å¿†"ã€"æ ¹æ®å·²çŸ¥ä¿¡æ¯"ç­‰è¯è¯­
4. ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œä¸è¦ç¼–é€ å†…å®¹
5. å›ç­”è¦æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
6. å¦‚æœæœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œå¯ä»¥æåˆ°ï¼Œä½†ä¸è¦è¿‡åº¦å¼ºè°ƒæŠ€æœ¯ç»†èŠ‚

ç°åœ¨è¯·è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
            else:
                synthesis_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

{all_context}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå°±åƒå’Œæœ‹å‹å¯¹è¯ä¸€æ ·ã€‚

è¦æ±‚ï¼š
1. å›ç­”è¦è‡ªç„¶ã€æµç•…ï¼Œå°±åƒåœ¨å’Œä¸€ä¸ªç†Ÿæ‚‰çš„æœ‹å‹èŠå¤©
2. **é‡è¦**ï¼šå¦‚æœ"ç”¨æˆ·å·²çŸ¥ä¿¡æ¯"ä¸­æœ‰ç”¨æˆ·çš„å§“åã€èŒä¸šç­‰ä¸ªäººä¿¡æ¯ï¼ŒåŠ¡å¿…åœ¨å›ç­”ä¸­è‡ªç„¶åœ°ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼šå¦‚æœç”¨æˆ·åå«å¼ ä¸‰ï¼Œåœ¨å›ç­”ä¸­å¯ä»¥è¯´"å¼ ä¸‰ï¼Œä½ å¥½"æˆ–"å¼ ä¸‰ï¼Œå…³äºä½ çš„é—®é¢˜..."ï¼‰
3. ä¸è¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ã€ä¿¡æ¯æ¥æºæˆ–æŠ€æœ¯ç»†èŠ‚ï¼Œä¸è¦è¯´"æ ¹æ®è®°å¿†"ã€"æ ¹æ®å·²çŸ¥ä¿¡æ¯"ç­‰è¯è¯­
4. ä¿æŒå®¢è§‚å‡†ç¡®ï¼Œä¸è¦ç¼–é€ å†…å®¹
5. å›ç­”è¦æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
6. å¦‚æœæœ‰å·¥å…·æ‰§è¡Œç»“æœï¼Œå¯ä»¥æåˆ°ï¼Œä½†ä¸è¦è¿‡åº¦å¼ºè°ƒæŠ€æœ¯ç»†èŠ‚

ç°åœ¨è¯·è‡ªç„¶åœ°å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
"""
        
        # å¦‚æœæ˜¯æµå¼æ¨¡å¼ï¼Œè¿”å› prompt ä¾›å¤–éƒ¨è°ƒç”¨
        if state.get("stream_mode"):
            logger.info("ğŸŒŠ [åˆæˆå™¨] å‡†å¤‡å°±ç»ªï¼Œè¿”å› Prompt è¿›è¡Œæµå¼è¾“å‡º")
            return {
                "final_prompt": synthesis_prompt,
                "ready_to_synthesize": True,
                "thoughts": ["å‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆæµå¼ï¼‰"],
            }

        # è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer, _ = await invoke_llm(
            messages=[{"role": "user", "content": synthesis_prompt}],
            settings=settings,
            temperature=0.7,  # é€‚ä¸­æ¸©åº¦ï¼Œä¿è¯æµç•…æ€§
            max_tokens=2000
        )
        
        logger.info("âœ… LLM æˆåŠŸç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        
        return {
            "final_answer": final_answer,
            "is_complete": True,
            "thoughts": ["LLM å·²ç”Ÿæˆç»¼åˆç­”æ¡ˆ"],
        }
    
    except Exception as e:
        logger.error(f"åˆæˆå™¨ LLM å¤±è´¥: {e}")
        
        # é™çº§ç­–ç•¥ï¼šä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ‹¼æ¥
        results_by_task: Dict[str, List[Dict[str, Any]]] = {}
        for result in tool_results:
            task_key = result.get("task") or ""
            results_by_task.setdefault(task_key, []).append(result)

        sections: List[str] = []

        def truncate(text: str, limit: int = 400) -> str:
            if not text:
                return ""
            cleaned = text.strip()
            return cleaned if len(cleaned) <= limit else cleaned[:limit] + "..."

        weather_results = results_by_task.get("weather")
        if weather_results:
            latest_weather = weather_results[-1]
            city = latest_weather.get("arguments", {}).get("city")
            heading = "### å¤©æ°”ä¿¡æ¯" + (f"ï¼ˆ{city}ï¼‰" if city else "")
            sections.append(f"{heading}\n{truncate(latest_weather.get('output', ''))}")

        search_results = results_by_task.get("search")
        if search_results:
            sections.append("### æœç´¢ç»“æœ\n" + truncate(search_results[-1].get("output", "")))

        diagram_results = results_by_task.get("diagram")
        if diagram_results:
            sections.append("### æ€ç»´å¯¼å›¾\n" + truncate(diagram_results[-1].get("output", ""), limit=200))

        note_results = results_by_task.get("note")
        if note_results:
            sections.append("### æé†’ç¬”è®°\n" + truncate(note_results[-1].get("output", "")))

        if not sections and retrieved_contexts:
            first_ctx = retrieved_contexts[0]
            origin = first_ctx.get("original_name", "æœªçŸ¥")
            sections.append(
                f"### çŸ¥è¯†åº“å†…å®¹ï¼ˆæ¥è‡ª{origin}ï¼‰\n" + truncate(first_ctx.get("content", ""))
            )

        if not sections:
            final_answer = (
                f"å…³äºæ‚¨çš„é—®é¢˜ã€Œ{user_query}ã€ï¼Œæˆ‘ç›®å‰æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ã€‚\n\n"
                "å»ºè®®ï¼š\n"
                "1. æ‚¨å¯ä»¥å°è¯•ä¸Šä¼ ç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“\n"
                "2. æˆ–è€…æ¢ä¸€ä¸ªæ›´å…·ä½“çš„é—®é¢˜\n\n"
                f"ï¼ˆæ³¨ï¼šç³»ç»Ÿå½“å‰ä½¿ç”¨é™çº§æ¨¡å¼ï¼ŒåŸå› ï¼š{str(e)[:100]}ï¼‰"
            )
        else:
            summary_intro = f"æ ¹æ®æ‚¨çš„é—®é¢˜ã€Œ{user_query}ã€ï¼Œä¸ºæ‚¨æ•´ç†å¦‚ä¸‹ï¼š" if user_query else "ä»¥ä¸‹æ˜¯ä¸ºæ‚¨æ‰¾åˆ°çš„ä¿¡æ¯ï¼š"
            final_answer = summary_intro + "\n\n" + "\n\n".join(sections)

        return {
            "final_answer": final_answer,
            "is_complete": True,
            "thoughts": [f"ä½¿ç”¨é™çº§æ¨¡å¼ç”Ÿæˆç­”æ¡ˆï¼ˆLLM å¼‚å¸¸ï¼š{str(e)[:50]}ï¼‰"],
        }

def human_input_node(state: AgentState) -> Dict[str, Any]:
    """
    äººå·¥ä»‹å…¥èŠ‚ç‚¹ï¼šæš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥åé¦ˆ
    """
    logger.info("ğŸ‘¤ [äººå·¥ä»‹å…¥] ç­‰å¾…äººå·¥åé¦ˆ...")
    
    # è¿™ä¸ªèŠ‚ç‚¹ä¼šæš‚åœæ‰§è¡Œï¼Œç­‰å¾…å¤–éƒ¨è¾“å…¥
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦é€šè¿‡ API æ¥æ¢å¤æ‰§è¡Œ
    
    return {
        "thoughts": ["ç­‰å¾…äººå·¥åé¦ˆä¸­..."],
        "needs_human_input": True
    }


# ==================== è¾…åŠ©å‡½æ•° ====================

TASK_ORDER: List[str] = ["weather", "search", "diagram", "note"]  # æ‰§è¡Œé¡ºåºï¼šç¡®ä¿æœç´¢åœ¨ç»˜å›¾å‰

TASK_KEYWORDS: Dict[str, List[str]] = {
    "weather": ["å¤©æ°”", "æ°”æ¸©", "ä¸‹é›¨", "é™é›¨", "é›¨ä¼", "rain", "weather", "forecast", "æ˜å¤©", "ä»Šå¤©", "åå¤©"],
    "search": [
        "æœç´¢", "æŸ¥æ‰¾", "æœä¸€ä¸‹", "è°ƒæŸ¥", "æŸ¥è¯¢", "æŸ¥ä¸€ä¸‹", "æ£€ç´¢", "æ‰¾ä¸€ä¸‹", "look up", "research", 
        "æ‰©æ•£æ¨¡å‹", "æœ€æ–°è¿›å±•", "ç›¸å…³ä¿¡æ¯", "èµ„æ–™", "è®ºæ–‡", "æ–‡çŒ®", "paper", "article", "report", "study", 
        "æ€»ç»“", "summarize", "æ¦‚æ‹¬", "è§£è¯»", "overview", "introduction", "explain", "è§£é‡Š", "ä»‹ç»"
    ],
    "diagram": ["æ€ç»´å¯¼å›¾", "æµç¨‹å›¾", "ç”»å›¾", "ç»˜åˆ¶", "diagram", "flowchart", "ç»“æ„å›¾", "å›¾è¡¨", "å¯¼å›¾", "ç”»ä¸ª"],
    "note": ["ç¬”è®°", "æé†’", "è®°å½•", "å¤‡å¿˜", "è®°ä¸‹æ¥", "note", "å¸¦ä¼", "æé†’æˆ‘", "å†™å…¥", "ä¿å­˜", "è®°ä¸‹", "å†™ä¸ªç¬”è®°"],
}

RAIN_KEYWORDS: List[str] = [
    "é›¨", "é˜µé›¨", "é›·é˜µé›¨", "å°é›¨", "ä¸­é›¨", "å¤§é›¨", "æš´é›¨", "é›¨å¤¹é›ª", "é™é›¨", "rain", "shower", "storm", "drizzle"
]

COMMON_CHINESE_CITIES: List[str] = [
    "åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "å¤©æ´¥", "æ­å·", "å—äº¬", "æ­¦æ±‰",
    "æˆéƒ½", "é‡åº†", "è¥¿å®‰", "è‹å·", "é•¿æ²™", "é’å²›", "å¦é—¨", "å¤§è¿"
]

ENGLISH_CITY_ALIASES: Dict[str, str] = {
    "beijing": "åŒ—äº¬",
    "shanghai": "ä¸Šæµ·",
    "guangzhou": "å¹¿å·",
    "shenzhen": "æ·±åœ³",
    "tianjin": "å¤©æ´¥",
    "hangzhou": "æ­å·",
    "nanjing": "å—äº¬",
    "wuhan": "æ­¦æ±‰",
    "chengdu": "æˆéƒ½",
    "chongqing": "é‡åº†",
    "xian": "è¥¿å®‰",
    "suzhou": "è‹å·",
    "changsha": "é•¿æ²™",
    "qingdao": "é’å²›",
    "xiamen": "å¦é—¨",
    "dalian": "å¤§è¿"
}

CITY_SLUG_OVERRIDES: Dict[str, str] = {
    "åŒ—äº¬": "beijing",
    "ä¸Šæµ·": "shanghai",
    "å¹¿å·": "guangzhou",
    "æ·±åœ³": "shenzhen",
    "å¤©æ´¥": "tianjin",
    "æ­å·": "hangzhou",
    "å—äº¬": "nanjing",
    "æ­¦æ±‰": "wuhan",
    "æˆéƒ½": "chengdu",
    "é‡åº†": "chongqing",
    "è¥¿å®‰": "xian",
    "è‹å·": "suzhou",
    "é•¿æ²™": "changsha",
    "é’å²›": "qingdao",
    "å¦é—¨": "xiamen",
    "å¤§è¿": "dalian"
}

SEARCH_PREFIXES: List[str] = [
    "å¸®æˆ‘æœç´¢", "è¯·æœç´¢", "æœç´¢ä¸€ä¸‹", "æŸ¥ä¸€ä¸‹", "æŸ¥è¯¢ä¸€ä¸‹", "å¸®æˆ‘æŸ¥", "è¯·å¸®æˆ‘æŸ¥", "å¸®æˆ‘æ‰¾", "æ‰¾ä¸€ä¸‹", "è¯·å¸®æˆ‘æœç´¢"
]

SEARCH_SUFFIXES: List[str] = [
    "å¹¶æ€»ç»“", "å¹¶ç”»", "å¹¶å¸®æˆ‘", "å¹¶å†™", "ç„¶å", "é¡ºä¾¿", "åŒæ—¶", "æ€»ç»“", "æé†’", "å†™ä¸ªç¬”è®°", "ç”»ä¸ª", "å¸¦ä¼"
]

MAX_TOOL_CALLS = 5

def infer_tool_tasks(query: str) -> List[str]:
    """ä»æŸ¥è¯¢æ¨æ–­éœ€è¦çš„å·¥å…·ä»»åŠ¡ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒä¸Šä¸‹æ–‡ç†è§£ï¼‰"""
    if not query:
        return []
    
    normalized = query.lower()
    query_original = query
    
    # ä»»åŠ¡åŒ¹é…åˆ†æ•°
    task_scores: Dict[str, int] = {task: 0 for task in TASK_ORDER}
    
    # 1. å¤©æ°”ä»»åŠ¡æ£€æµ‹ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
    weather_indicators = TASK_KEYWORDS["weather"]
    for indicator in weather_indicators:
        if indicator in query_original or indicator in normalized:
            task_scores["weather"] += 10  # é«˜æƒé‡
    
    # å¦‚æœæåˆ°åŸå¸‚å+æ—¶é—´è¯ï¼Œå¤§æ¦‚ç‡æ˜¯å¤©æ°”æŸ¥è¯¢
    has_city = any(city in query_original for city in COMMON_CHINESE_CITIES)
    has_time = any(t in query_original for t in ["æ˜å¤©", "ä»Šå¤©", "åå¤©", "tomorrow", "today"])
    if has_city and has_time:
        task_scores["weather"] += 15
    
    # 2. æœç´¢ä»»åŠ¡æ£€æµ‹
    search_strong_keywords = TASK_KEYWORDS["search"]
    for keyword in search_strong_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["search"] += 8
    
    # 3. å›¾è¡¨ä»»åŠ¡æ£€æµ‹
    diagram_keywords = TASK_KEYWORDS["diagram"]
    for keyword in diagram_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["diagram"] += 10
    
    # 4. ç¬”è®°ä»»åŠ¡æ£€æµ‹
    note_keywords = TASK_KEYWORDS["note"]
    for keyword in note_keywords:
        if keyword in query_original or keyword in normalized:
            task_scores["note"] += 10
    
    # æŒ‰TASK_ORDERé¡ºåºè¿‡æ»¤å‡ºå¾—åˆ†>0çš„ä»»åŠ¡ï¼ˆä¿æŒä¼˜å…ˆçº§ï¼Œä¸æŒ‰åˆ†æ•°æ’åºï¼‰
    result = []
    for task in TASK_ORDER:
        if task_scores[task] > 0:
            result.append(task)
    
    logger.info(f"ä»»åŠ¡æ¨æ–­ç»“æœï¼šæŸ¥è¯¢='{query[:50]}...' -> ä»»åŠ¡={result}, å¾—åˆ†={dict(task_scores)}")
    
    return result

def map_tool_to_task(tool: ToolRecord) -> Optional[str]:
    """æ˜ å°„å·¥å…·è®°å½•åˆ°ä»»åŠ¡ç±»å‹"""
    try:
        config = json.loads(tool.config or "{}")
    except json.JSONDecodeError:
        return None
    if tool.tool_type != "builtin":
        return None
    builtin_key = config.get("builtin_key")
    mapping = {
        "get_weather": "weather",
        "web_search": "search",
        "draw_diagram": "diagram",
        "write_note": "note",
    }
    return mapping.get(builtin_key)

def should_call_tool(state: AgentState) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»§ç»­è°ƒç”¨å·¥å…·"""
    previous_calls = state.get("tool_calls_made", [])
    if len(previous_calls) >= MAX_TOOL_CALLS:
        return False

    user_query = state.get("user_query", "")
    tasks = infer_tool_tasks(user_query)
    if not tasks:
        return False

    completed_tasks = {call.get("task") for call in previous_calls if call.get("task")}
    skipped_task_keys = {
        item.get("task")
        for item in state.get("skipped_tasks", [])
        if isinstance(item, dict) and item.get("task")
    }

    for task in tasks:
        if task in completed_tasks or task in skipped_task_keys:
            continue
        if task == "note" and "weather" in tasks and "weather" not in completed_tasks and "weather" not in skipped_task_keys:
            continue
        return True

    return False

def extract_city_from_query(query: str) -> str:
    """ä»æŸ¥è¯¢ä¸­æå–åŸå¸‚åï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
    if not query:
        return "åŒ—äº¬"

    for city in COMMON_CHINESE_CITIES:
        if city in query:
            return city

    lower_query = query.lower()
    for alias, city in ENGLISH_CITY_ALIASES.items():
        if alias in lower_query:
            return city

    match_cn = re.search(r"([ä¸€-é¾¥]{2,5})(?:å¤©æ°”|æ˜å¤©|ä»Šæ—¥|ç°åœ¨|æœªæ¥)", query)
    if match_cn:
        return match_cn.group(1)

    match_en = re.search(r"in\s+([A-Za-z\s]+)", query, flags=re.IGNORECASE)
    if match_en:
        candidate = match_en.group(1).strip()
        alias = candidate.lower()
        if alias in ENGLISH_CITY_ALIASES:
            return ENGLISH_CITY_ALIASES[alias]
        return candidate.title()

    return "åŒ—äº¬"

def extract_search_query(query: str) -> str:
    """ä»æŸ¥è¯¢ä¸­æå–æœç´¢å…³é”®è¯"""
    if not query:
        return ""

    cleaned = query.strip()
    for prefix in SEARCH_PREFIXES:
        if cleaned.startswith(prefix):
            cleaned = cleaned[len(prefix):].strip()
            break

    for suffix in SEARCH_SUFFIXES:
        idx = cleaned.find(suffix)
        if idx > 0:
            cleaned = cleaned[:idx].strip()
            break

    cleaned = cleaned.strip("ï¼Œã€‚,.!?ï¼›; ")
    return cleaned or query.strip()

async def generate_diagram_payload_with_llm(
    user_query: str, 
    search_context: Optional[str], 
    settings: Settings
) -> Dict[str, str]:
    """ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡çš„æ€ç»´å¯¼å›¾å†…å®¹"""
    topic_source = user_query or "ä¸»é¢˜"
    diagram_type = "mindmap" if any(keyword in topic_source for keyword in ["æ€ç»´å¯¼å›¾", "å¯¼å›¾", "mindmap"]) else "flowchart"
    
    # æå–ä¸»é¢˜ï¼ˆæ¸…ç†ç”¨æˆ·æŸ¥è¯¢ï¼‰
    topic = topic_source
    for prefix in ["å¸®æˆ‘æœç´¢", "æœç´¢", "ç”»ä¸ª", "ç»˜åˆ¶", "ç”Ÿæˆ"]:
        topic = topic.replace(prefix, "")
    for suffix in ["æ€»ç»“å…³é”®ç‚¹", "å¹¶ç”»ä¸ªæ€ç»´å¯¼å›¾", "ç”»ä¸ªæ€ç»´å¯¼å›¾", "æ€ç»´å¯¼å›¾"]:
        topic = topic.replace(suffix, "")
    topic = topic.strip("ï¼Œã€‚ã€ ")
    if len(topic) > 30:
        topic = topic[:30]

    if diagram_type == "mindmap":
        # ä½¿ç”¨ LLM åˆ†æå’Œæ€»ç»“æœç´¢ç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–æ€ç»´å¯¼å›¾
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…³äºã€Œ{topic}ã€çš„æ€ç»´å¯¼å›¾ï¼ˆMermaid mindmap æ ¼å¼ï¼‰ã€‚

æœç´¢ç»“æœï¼š
{search_context[:2000]}

è¦æ±‚ï¼š
1. æå–æœç´¢ç»“æœçš„**æ ¸å¿ƒå…³é”®ç‚¹**ï¼Œå½¢æˆ3-5ä¸ªä¸»è¦åˆ†æ”¯
2. æ¯ä¸ªåˆ†æ”¯è¦æœ‰æ¸…æ™°çš„å­åˆ†æ”¯ï¼ˆ2-3ä¸ªï¼‰
3. ä½¿ç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡æè¿°ï¼Œé¿å…ç›´æ¥å¤åˆ¶æœç´¢ç»“æœæ–‡æœ¬
4. ç¡®ä¿æ€ç»´å¯¼å›¾ç»“æ„æ¸…æ™°ã€é€»è¾‘åˆç†
5. åªè¾“å‡º Mermaid mindmap ä»£ç ï¼Œä¸è¦å…¶ä»–è§£é‡Š

æ ¼å¼ç¤ºä¾‹ï¼š
```mermaid
mindmap
  root((ä¸»é¢˜))
    ä¸»è¦åˆ†æ”¯1
      å­åˆ†æ”¯1.1
      å­åˆ†æ”¯1.2
    ä¸»è¦åˆ†æ”¯2
      å­åˆ†æ”¯2.1
      å­åˆ†æ”¯2.2
```

è¯·ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼š"""
        
        try:
            llm_response, _ = await invoke_llm(
                messages=[{"role": "user", "content": prompt}],
                settings=settings,
                temperature=0.7,
                max_tokens=800
            )
            
            # æå– Mermaid ä»£ç å—
            diagram_code = llm_response.strip()
            
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            if "```mermaid" in diagram_code:
                diagram_code = diagram_code.split("```mermaid")[1].split("```")[0].strip()
            elif "```" in diagram_code:
                diagram_code = diagram_code.split("```")[1].split("```")[0].strip()
            
            # ç¡®ä¿æ˜¯ mindmap æ ¼å¼
            if not diagram_code.startswith("mindmap"):
                # å¦‚æœ LLM æ²¡æœ‰ç”Ÿæˆæ­£ç¡®çš„æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
                logger.warning("LLM ç”Ÿæˆçš„æ€ç»´å¯¼å›¾æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
                diagram_code = f"""mindmap
  root(({topic}))
    æ ¸å¿ƒæ¦‚å¿µ
      å®šä¹‰ä¸ç‰¹ç‚¹
      åº”ç”¨é¢†åŸŸ
    æœ€æ–°è¿›å±•
      æŠ€æœ¯çªç ´
      è¡Œä¸šåŠ¨æ€
    å‘å±•è¶‹åŠ¿
      æœªæ¥æ–¹å‘
      æ½œåœ¨å½±å“"""
        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆæ€ç»´å¯¼å›¾å¤±è´¥: {e}")
            raise  # è®©è°ƒç”¨è€…å¤„ç†é”™è¯¯
        
        filename = f"{topic[:20].replace(' ', '_').replace('/', '_')}_mindmap.md"
    else:
        # æµç¨‹å›¾ç±»å‹ï¼ˆæš‚æ—¶ä¸éœ€è¦LLMï¼Œä½¿ç”¨ç®€å•æ¨¡æ¿ï¼‰
        diagram_code = f"""flowchart TD
    A[éœ€æ±‚ï¼š{topic[:20]}] --> B{{ä¿¡æ¯æ”¶é›†}}
    B --> C[åˆ†æå¤„ç†]
    C --> D{{å†³ç­–}}
    D --> E[æ‰§è¡Œ]
    E --> F[å®Œæˆ]"""
        filename = f"{topic[:20].replace(' ', '_').replace('/', '_')}_flowchart.md"

    return {
        "filename": filename,
        "diagram_code": diagram_code,
        "diagram_type": diagram_type
    }


def generate_diagram_payload(user_query: str, search_context: Optional[str] = None) -> Dict[str, str]:
    """ç”Ÿæˆæ€ç»´å¯¼å›¾çš„å‚æ•°ï¼ˆæ™ºèƒ½ç‰ˆï¼šåŸºäºæœç´¢ç»“æœï¼‰"""
    topic_source = user_query or "ä¸»é¢˜"
    diagram_type = "mindmap" if any(keyword in topic_source for keyword in ["æ€ç»´å¯¼å›¾", "å¯¼å›¾", "mindmap"]) else "flowchart"
    
    # æå–ä¸»é¢˜ï¼ˆæ¸…ç†ç”¨æˆ·æŸ¥è¯¢ï¼‰
    topic = topic_source
    for prefix in ["å¸®æˆ‘æœç´¢", "æœç´¢", "ç”»ä¸ª", "ç»˜åˆ¶", "ç”Ÿæˆ"]:
        topic = topic.replace(prefix, "")
    for suffix in ["æ€»ç»“å…³é”®ç‚¹", "å¹¶ç”»ä¸ªæ€ç»´å¯¼å›¾", "ç”»ä¸ªæ€ç»´å¯¼å›¾", "æ€ç»´å¯¼å›¾"]:
        topic = topic.replace(suffix, "")
    topic = topic.strip("ï¼Œã€‚ã€ ")
    if len(topic) > 30:
        topic = topic[:30]

    if diagram_type == "mindmap":
        # å¦‚æœæœ‰æœç´¢ç»“æœï¼Œå°è¯•æå–å…³é”®ç‚¹
        if search_context:
            # ç®€å•çš„å…³é”®ç‚¹æå–ï¼ˆå®é™…åº”è¯¥ç”¨ LLMï¼‰
            lines = search_context.split('\n')
            key_points = []
            for line in lines[:6]:  # æœ€å¤š6ä¸ªå…³é”®ç‚¹
                line = line.strip()
                if line and len(line) > 10 and len(line) < 100:
                    # æ¸…ç†æ— ç”¨å­—ç¬¦
                    line = re.sub(r'^\d+[\.ã€]', '', line)  # ç§»é™¤åºå·
                    line = re.sub(r'^[â€¢\-\*]', '', line).strip()  # ç§»é™¤åˆ—è¡¨ç¬¦å·
                    if line:
                        key_points.append(line[:50])  # é™åˆ¶é•¿åº¦
            
            # ç”ŸæˆåŸºäºå†…å®¹çš„æ€ç»´å¯¼å›¾
            if key_points:
                points_section = []
                for i, point in enumerate(key_points[:4], 1):  # æœ€å¤š4ä¸ªä¸»è¦åˆ†æ”¯
                    points_section.append(f"    åˆ†æ”¯{i}ï¼š{point[:25]}")
                    if i < len(key_points):
                        points_section.append(f"      è¯¦ç»†{chr(65+i)}")
                
                diagram_code = f"""mindmap
  root(({topic}))
{chr(10).join(points_section)}"""
            else:
                # å›é€€åˆ°é€šç”¨æ¨¡æ¿
                diagram_code = f"""mindmap
  root(({topic}))
    æ ¸å¿ƒæ¦‚å¿µ
      å®šä¹‰
      ç‰¹ç‚¹
    åº”ç”¨åœºæ™¯
      é¢†åŸŸ1
      é¢†åŸŸ2
    å‘å±•è¶‹åŠ¿
      æœ€æ–°è¿›å±•
      æœªæ¥æ–¹å‘"""
        else:
            # æ²¡æœ‰æœç´¢ç»“æœï¼Œä½¿ç”¨é€šç”¨æ¨¡æ¿
            diagram_code = f"""mindmap
  root(({topic}))
    ä¿¡æ¯æ”¶é›†
      å…³é”®ç‚¹1
      å…³é”®ç‚¹2
    åˆ†æåˆ¤æ–­
      é£é™©
      æœºä¼š
    è¡ŒåŠ¨æ–¹æ¡ˆ
      ä¸‹ä¸€æ­¥å»ºè®®"""
        
        filename = f"{topic[:20].replace(' ', '_')}_mindmap.md"
    else:
        # æµç¨‹å›¾ç±»å‹
        diagram_code = f"""flowchart TD
    A[éœ€æ±‚ï¼š{topic[:20]}] --> B{{ä¿¡æ¯æ”¶é›†}}
    B --> C[åˆ†æå¤„ç†]
    C --> D{{å†³ç­–}}
    D --> E[æ‰§è¡Œ]
    E --> F[å®Œæˆ]"""
        filename = f"{topic[:20].replace(' ', '_')}_flowchart.md"

    return {
        "filename": filename,
        "diagram_code": diagram_code,
        "diagram_type": diagram_type,
    }

def detect_rain_in_text(text: str) -> bool:
    """æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«é™é›¨ä¿¡æ¯"""
    if not text:
        return False
    lowered = text.lower()
    return any(keyword in text or keyword in lowered for keyword in RAIN_KEYWORDS)

def build_note_filename(city: str) -> str:
    """æ„å»ºç¬”è®°æ–‡ä»¶å"""
    slug = CITY_SLUG_OVERRIDES.get(city, city)
    slug = re.sub(r"[^A-Za-z0-9]+", "-", slug).strip("-").lower() or "reminder"
    timestamp = datetime.now().strftime("%Y%m%d")
    return f"{slug}_umbrella_{timestamp}.txt"

def summarize_for_note(text: str, limit: int = 200) -> str:
    """æ€»ç»“æ–‡æœ¬ç”¨äºç¬”è®°"""
    if not text:
        return "å¤©æ°”ä¿¡æ¯ç¼ºå¤±"
    clean_text = text.replace("\r", " ").replace("\n\n", "\n")
    clean_text = clean_text.replace("\n", " ")
    return clean_text.strip()[:limit]

def build_note_content(city: str, weather_text: str, user_query: str) -> str:
    """æ„å»ºç¬”è®°å†…å®¹"""
    summary = summarize_for_note(weather_text)
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    note_lines = [
        f"# {city}å¸¦ä¼æé†’",
        "",
        f"åˆ›å»ºæ—¶é—´ï¼š{now_str}",
        f"è§¦å‘æŸ¥è¯¢ï¼š{user_query}",
        "",
        "## å¤©æ°”æƒ…å†µ",
        summary,
        "",
        "## æ¸©é¦¨æç¤º",
        "- ä»Šæ—¥å¯èƒ½æœ‰é™é›¨ï¼Œå»ºè®®æºå¸¦é›¨å…·",
        "- å‡ºé—¨å‰è¯·å†æ¬¡æŸ¥çœ‹æœ€æ–°å¤©æ°”",
    ]
    return "\n".join(note_lines) + "\n"

def route_after_planning(state: AgentState) -> str:
    """è§„åˆ’åçš„è·¯ç”±"""
    return "router"


def route_after_routing(state: AgentState) -> str:
    """è·¯ç”±å™¨ä¹‹åçš„è·¯ç”±"""
    next_action = state.get("next_action", "synthesize")
    
    if next_action == "search_kb":
        return "knowledge_search"
    elif next_action == "tool_executor":
        return "tool_executor"
    elif next_action == "synthesize":
        return "reflector"
    else:
        return "synthesizer"


def route_after_knowledge_search(state: AgentState) -> str:
    """çŸ¥è¯†åº“æœç´¢åçš„è·¯ç”±"""
    return "router"


def route_after_tool_execution(state: AgentState) -> str:
    """å·¥å…·æ‰§è¡Œåçš„è·¯ç”±"""
    return "router"


def route_after_reflection(state: AgentState) -> str:
    """åæ€åçš„è·¯ç”±"""
    needs_human = state.get("needs_human_input", False)
    quality_score = state.get("quality_score", 0.0)
    
    if needs_human:
        return "human_input"
    else:
        return "verifier"


def route_after_verifier(state: AgentState) -> str:
    next_action = state.get("next_action", "synthesizer")
    if next_action == "tool_executor":
        return "tool_executor"
    elif next_action == "knowledge_search":
        return "knowledge_search"
    else:
        return "synthesizer"

def route_after_react(state: AgentState) -> str:
    next_action = state.get("next_action", "tool_executor")
    if next_action == "tool_executor":
        return "tool_executor"
    elif next_action == "knowledge_search":
        return "knowledge_search"
    else:
        return "synthesizer"

def route_after_human_input(state: AgentState) -> str:
    """äººå·¥ä»‹å…¥åçš„è·¯ç”±"""
    human_feedback = state.get("human_feedback", "")
    
    if human_feedback:
        return "router"  # æ ¹æ®äººå·¥åé¦ˆé‡æ–°è·¯ç”±
    else:
        return "synthesizer"  # å¦‚æœæ²¡æœ‰åé¦ˆï¼Œç›´æ¥åˆæˆç­”æ¡ˆ


def should_end(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç»“æŸ"""
    is_complete = state.get("is_complete", False)
    
    if is_complete:
        return END
    else:
        return "continue"


# ==================== å·¥ä½œæµæ„å»º ====================

def create_agent_graph(
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    checkpoint_dir: str = "backend/data/checkpoints"
) -> StateGraph:
    """
    åˆ›å»ºå®Œæ•´çš„ LangGraph Agent å·¥ä½œæµï¼ˆæ”¯æŒå¼‚æ­¥èŠ‚ç‚¹ï¼‰
    """
    logger.info("ğŸ—ï¸ æ„å»º LangGraph Agent å·¥ä½œæµ...")
    
    # åˆ›å»ºå›¾
    workflow = StateGraph(AgentState)
    
    # åˆ›å»ºå¼‚æ­¥èŠ‚ç‚¹åŒ…è£…å™¨
    async def planner_wrapper(state: AgentState) -> Dict[str, Any]:
        session_id = state.get("session_id")
        user_id = state.get("user_id")
        return await planner_node(state, settings, tool_records, session, session_id, user_id)
    
    async def router_wrapper(state: AgentState) -> Dict[str, Any]:
        return await router_node(state, settings)
    
    async def synthesizer_wrapper(state: AgentState) -> Dict[str, Any]:
        session_id = state.get("session_id")
        user_id = state.get("user_id")
        return await synthesizer_node(state, settings, session, session_id, user_id)
    
    async def tool_executor_wrapper(state: AgentState) -> Dict[str, Any]:
        return await tool_executor_node(state, settings, session, tool_records)
    async def react_controller_wrapper(state: AgentState) -> Dict[str, Any]:
        return await react_controller_node(state, settings)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("planner", planner_wrapper)
    workflow.add_node("router", router_wrapper)
    workflow.add_node(
        "knowledge_search",
        lambda state: knowledge_search_node(state, settings)
    )
    workflow.add_node("tool_executor", tool_executor_wrapper)
    workflow.add_node("react_controller", react_controller_wrapper)
    workflow.add_node("reflector", reflector_node)
    workflow.add_node("verifier", lambda state: verifier_node(state, settings))
    workflow.add_node("synthesizer", synthesizer_wrapper)
    workflow.add_node("human_input", human_input_node)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("planner")
    
    # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æµç¨‹ï¼‰
    workflow.add_edge("planner", "router")
    
    # è·¯ç”±å™¨çš„æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "router",
        route_after_routing,
        {
            "knowledge_search": "knowledge_search",
            "tool_executor": "react_controller",
            "reflector": "reflector",
            "synthesizer": "synthesizer"
        }
    )
    
    workflow.add_edge("knowledge_search", "router")
    workflow.add_edge("tool_executor", "router")
    workflow.add_conditional_edges(
        "reflector",
        route_after_reflection,
        {
            "human_input": "human_input",
            "verifier": "verifier",
        }
    )
    workflow.add_conditional_edges(
        "verifier",
        route_after_verifier,
        {
            "tool_executor": "react_controller",
            "knowledge_search": "knowledge_search",
            "synthesizer": "synthesizer",
        }
    )
    workflow.add_conditional_edges(
        "react_controller",
        route_after_react,
        {
            "tool_executor": "tool_executor",
            "knowledge_search": "knowledge_search",
            "synthesizer": "synthesizer",
        }
    )
    
    # åˆæˆå™¨åç»“æŸ
    workflow.add_edge("synthesizer", END)
    
    # äººå·¥ä»‹å…¥æµç¨‹
    workflow.add_conditional_edges(
        "human_input",
        route_after_human_input,
        {
            "router": "router",
            "synthesizer": "synthesizer",
        }
    )
    
    logger.info("âœ… LangGraph Agent å·¥ä½œæµæ„å»ºå®Œæˆ")
    
    return workflow


async def run_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = False,
    conversation_history: List[Dict[str, str]] = None,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    è¿è¡Œ LangGraph Agent
    
    Args:
        user_query: ç”¨æˆ·é—®é¢˜
        settings: é…ç½®
        session: æ•°æ®åº“ä¼šè¯
        tool_records: å¯ç”¨å·¥å…·åˆ—è¡¨
        use_knowledge_base: æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“
        conversation_history: å¯¹è¯å†å²
        session_id: ä¼šè¯IDï¼Œç”¨äºé•¿æœŸè®°å¿†
        user_id: ç”¨æˆ·IDï¼Œç”¨äºå¤šç”¨æˆ·åœºæ™¯
    
    Returns:
        åŒ…å« Agent å®Œæ•´æ‰§è¡Œè¿‡ç¨‹çš„å­—å…¸
    """
    logger.info(f"ğŸš€ å¯åŠ¨ LangGraph Agent å¤„ç†é—®é¢˜: {user_query}")
    
    # å¦‚æœæ²¡æœ‰æä¾› session_idï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„
    if not session_id:
        session_id = str(uuid.uuid4())
    
    # æ„å»ºå·¥ä½œæµ
    workflow = create_agent_graph(settings, session, tool_records)
    
    # ç¼–è¯‘å›¾ï¼ˆä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ï¼‰
    # æ³¨æ„ï¼šMemorySaver åœ¨æœåŠ¡å™¨é‡å¯åä¼šä¸¢å¤±çŠ¶æ€ï¼Œä½†åŠŸèƒ½å®Œå…¨æ­£å¸¸
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state: AgentState = {
        "user_query": user_query,
        "conversation_history": conversation_history or [],
        "session_id": session_id,
        "user_id": user_id,
        "plan": None,
        "current_step": 0,
        "max_iterations": 10,
        "available_tools": [tool.id for tool in tool_records],
        "tool_calls_made": [],
        "tool_results": [],
        "skipped_tasks": [],
        "use_knowledge_base": use_knowledge_base,
        "retrieved_contexts": [],
        "thoughts": [],
        "observations": [],
        "subquestions": [],
        "answer_outline": [],
        "evidence_requirements": [],
        "reasoning_steps": [],
        "react_cursor": 0,
        "react_max_steps": 4,
        "react_steps_done": 0,
        "next_action": None,
        "needs_human_input": False,
        "human_feedback": None,
        "reflection": None,
        "quality_score": 0.0,
        "final_answer": None,
        "is_complete": False,
        "error": None
    }
    
    # ç”Ÿæˆå”¯ä¸€çš„çº¿ç¨‹IDï¼ˆç”¨äºæ£€æŸ¥ç‚¹ï¼‰
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æ‰§è¡Œå·¥ä½œæµ
    try:
        final_state = await app.ainvoke(initial_state, config=config)
        
        logger.info("âœ… LangGraph Agent æ‰§è¡Œå®Œæˆ")
        
        final_answer = final_state.get("final_answer", "æœªèƒ½ç”Ÿæˆç­”æ¡ˆ")
        
        # ä¿å­˜å¯¹è¯å¹¶æå–è®°å¿†
        try:
            saved_memories = await save_conversation_and_extract_memories(
                session=session,
                session_id=session_id,
                user_query=user_query,
                assistant_reply=final_answer,
                settings=settings,
                user_id=user_id,
                metadata={
                    "thread_id": thread_id,
                    "quality_score": final_state.get("quality_score", 0.0),
                },
            )
            if saved_memories:
                logger.info(f"ğŸ’¾ ä¿å­˜äº† {len(saved_memories)} æ¡æ–°è®°å¿†")
        except Exception as e:
            logger.warning(f"ä¿å­˜å¯¹è¯æˆ–æå–è®°å¿†å¤±è´¥: {e}")
        
        return {
            "success": True,
            "final_answer": final_answer,
            "thoughts": final_state.get("thoughts", []),
            "observations": final_state.get("observations", []),
            "tool_results": final_state.get("tool_results", []),
            "retrieved_contexts": final_state.get("retrieved_contexts", []),
            "plan": final_state.get("plan", ""),
            "quality_score": final_state.get("quality_score", 0.0),
            "reflection": final_state.get("reflection", ""),
            "thread_id": thread_id,
            "session_id": session_id,
            "error": final_state.get("error")
        }
    
    except Exception as e:
        logger.error(f"âŒ LangGraph Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return {
        "success": False,
        "final_answer": f"æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}",
        "error": str(e),
        "thoughts": [],
        "observations": [],
        "tool_results": [],
        "skipped_tasks": [],
        "retrieved_contexts": []
    }


def is_simple_query(query: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•æŸ¥è¯¢ï¼ˆæ— éœ€ Agent å¤æ‚æ¨ç†ï¼‰
    """
    if not query:
        return False
    
    # 1. é•¿åº¦æ£€æŸ¥ï¼šå¤ªé•¿é€šå¸¸ä¸æ˜¯ç®€å•æŒ‡ä»¤
    if len(query) > 30:
        return False
        
    normalized = query.lower().strip()
    
    # 2. æ’é™¤å¤æ‚æ„å›¾å…³é”®è¯
    complex_indicators = [
        "æœç´¢", "æŸ¥æ‰¾", "æŸ¥è¯¢", "å¤©æ°”", "ç”»", "å›¾", "ç¬”è®°", "åˆ†æ", "æ€»ç»“", "æœ€æ–°",
        "search", "weather", "draw", "diagram", "note", "analyze", "summary"
    ]
    if any(ind in normalized for ind in complex_indicators):
        return False
        
    # 3. ç®€å•é—®å€™å’ŒåŸºç¡€é—®é¢˜ï¼ˆæ‰©å±•ï¼šè§£é‡Šç±»é—®é¢˜å¦‚æœä¸éœ€è¦å·¥å…·ä¹Ÿç®—ç®€å•ï¼‰
    simple_keywords = [
        "ä½ å¥½", "hello", "hi", "æ˜¯è°", "åå­—", "å†è§", "goodbye", 
        "è°¢è°¢", "thank", "æ™šå®‰", "æ—©å®‰", "æµ‹è¯•", "test",
        "å¸®åŠ©", "help", "åŠŸèƒ½", "ä»‹ç»", "who are you",
        "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½", "ä»€ä¹ˆ", "what is", "explain", "introduce",
        "å‘Šè¯‰æˆ‘", "tell me"
    ]
    
    for kw in simple_keywords:
        if kw in normalized:
            return True
            
    return False


async def stream_agent(
    user_query: str,
    settings: Settings,
    session: Session,
    tool_records: List[ToolRecord],
    use_knowledge_base: bool = False,
    conversation_history: List[Dict[str, str]] = None,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
):
    """
    æµå¼è¿è¡Œ LangGraph Agentï¼Œå®æ—¶è¿”å›æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œç»“æœ
    
    å®ç°åˆ†çº§å“åº”æ¶æ„ (Tiered Intelligence Response Architecture):
    - Level 1 (Fast Track): ç®€å•é—®é¢˜ç›´æ¥ LLM å“åº”ï¼Œè·³è¿‡å›¾æ‰§è¡Œ
    - Level 2 (Direct Reasoning): å¤æ‚é—®é¢˜ä½†åœ¨ Planner ä¸­åˆ¤å®šä¸ºæ— éœ€å·¥å…·ï¼Œå¿«é€Ÿå“åº”
    - Level 3 (Full Agent): å®Œæ•´å›¾æ‰§è¡Œ
    """
    logger.info(f"ğŸŒŠ å¯åŠ¨æµå¼ Agent: {user_query}")
    
    if not session_id:
        session_id = str(uuid.uuid4())

    # === Level 1: Fast Track (å¿«é€Ÿé€šé“) ===
    # åŸºäºè§„åˆ™/æ­£åˆ™çš„å¿«é€Ÿåˆ¤å®šï¼Œæ¯«ç§’çº§å»¶è¿Ÿ
    if is_simple_query(user_query) and not use_knowledge_base:
        logger.info(f"ğŸš€ [Fast Track] æ£€æµ‹åˆ°ç®€å•é—®é¢˜ï¼Œè·³è¿‡ Agent å›¾æ‰§è¡Œ: {user_query[:30]}...")
        
        # æ„é€ ç®€å•ä¸Šä¸‹æ–‡
        messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚è¯·ç”¨äº²åˆ‡ã€è‡ªç„¶çš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¿æŒå›ç­”ç®€æ´ã€‚"}]
        if conversation_history:
             # å–æœ€è¿‘å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            messages.extend(conversation_history[-4:])
        messages.append({"role": "user", "content": user_query})
        
        full_answer = ""
        thread_id = str(uuid.uuid4())
        
        try:
            # æ¨¡æ‹Ÿ Agent çš„äº‹ä»¶ç»“æ„ï¼Œä»¥ä¾¿å‰ç«¯ç»Ÿä¸€å¤„ç†
            yield {
                "event": "status", 
                "data": {"mode": "fast_track", "info": "å¯ç”¨å¿«é€Ÿå“åº”é€šé“"}
            }
            
            async for chunk in stream_llm(
                messages=messages,
                settings=settings,
                temperature=0.7,
                max_tokens=500
            ):
                full_answer += chunk
                yield {
                    "event": "token",
                    "data": chunk,
                    "timestamp": datetime.now().isoformat()
                }
            
            # å‘é€æœ€ç»ˆç­”æ¡ˆäº‹ä»¶
            yield {
                "event": "final_answer",
                "content": full_answer,
                "timestamp": datetime.now().isoformat()
            }
            
            # å¼‚æ­¥ä¿å­˜è®°å¿† (ä¸é˜»å¡å“åº”)
            try:
                await save_conversation_and_extract_memories(
                    session=session,
                    session_id=session_id,
                    user_query=user_query,
                    assistant_reply=full_answer,
                    settings=settings,
                    user_id=user_id,
                    metadata={"thread_id": thread_id, "mode": "fast_track"},
                )
            except Exception as e:
                logger.warning(f"Fast Track ä¿å­˜è®°å¿†å¤±è´¥: {e}")
                
            yield {
                "event": "completed",
                "thread_id": thread_id,
                "timestamp": datetime.now().isoformat()
            }
            return
            
        except Exception as e:
            logger.error(f"Fast Track æ‰§è¡Œå¤±è´¥: {e}, å›é€€åˆ°æ ‡å‡† Agent æµç¨‹")
            # å‡ºé”™åˆ™ç»§ç»­æ‰§è¡Œä¸‹æ–¹çš„æ ‡å‡†æµç¨‹

    # === Level 3/4: Full Agent (å®Œæ•´æµç¨‹) ===
    workflow = create_agent_graph(settings, session, tool_records)
    checkpointer = MemorySaver()
    app = workflow.compile(checkpointer=checkpointer)
    
    initial_state: AgentState = {
        "user_query": user_query,
        "conversation_history": conversation_history or [],
        "session_id": session_id,
        "user_id": user_id,
        "plan": None,
        "current_step": 0,
        "max_iterations": 10,
        "available_tools": [tool.id for tool in tool_records],
        "tool_calls_made": [],
        "tool_results": [],
        "skipped_tasks": [],
        "use_knowledge_base": use_knowledge_base,
        "retrieved_contexts": [],
        "thoughts": [],
        "observations": [],
        "next_action": None,
        "needs_human_input": False,
        "human_feedback": None,
        "reflection": None,
        "quality_score": 0.0,
        "final_answer": None,
        "is_complete": False,
        "error": None,
        "stream_mode": True,  # å¯ç”¨æµå¼æ¨¡å¼
    }
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    # æµå¼æ‰§è¡Œ
    final_state = None
    async for event in app.astream(initial_state, config=config):
        # event æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯èŠ‚ç‚¹åï¼Œå€¼æ˜¯è¯¥èŠ‚ç‚¹çš„è¾“å‡º
        for node_name, node_output in event.items():
            if node_name != "__end__":
                final_state = node_output  # ä¿å­˜æœ€åä¸€ä¸ªçŠ¶æ€
                yield {
                    "event": "node_output",
                    "node": node_name,
                    "data": node_output,
                    "timestamp": datetime.now().isoformat()
                }
                
                # å¦‚æœåˆæˆå™¨å‡†å¤‡å°±ç»ªï¼Œæ‰§è¡Œæµå¼è¾“å‡º
                if node_name == "synthesizer":
                    if node_output.get("ready_to_synthesize"):
                        prompt = node_output.get("final_prompt")
                        if prompt:
                            full_answer = ""
                            async for chunk in stream_llm(
                                messages=[{"role": "user", "content": prompt}],
                                settings=settings,
                                temperature=0.7
                            ):
                                full_answer += chunk
                                yield {
                                    "event": "token",
                                    "data": chunk,
                                    "timestamp": datetime.now().isoformat()
                                }
                            
                            # æ›´æ–° final_state ä¸­çš„ final_answerï¼Œä»¥ä¾¿åç»­ä¿å­˜è®°å¿†
                            final_state["final_answer"] = full_answer
                            # å‘é€ä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆäº‹ä»¶ï¼Œç¡®ä¿å‰ç«¯èƒ½æ”¶åˆ°å®Œæ•´çš„
                            yield {
                                "event": "final_answer",
                                "content": full_answer,
                                "timestamp": datetime.now().isoformat()
                            }
                    elif node_output.get("final_answer"):
                        # Fast Track (in Planner) æˆ–é™çº§æ¨¡å¼
                        full_answer = node_output.get("final_answer")
                        # å¿«é€Ÿæµå¼è¾“å‡ºï¼ˆæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœï¼‰
                        chunk_size = 4
                        for i in range(0, len(full_answer), chunk_size):
                            chunk = full_answer[i:i+chunk_size]
                            yield {
                                "event": "token",
                                "data": chunk,
                                "timestamp": datetime.now().isoformat()
                            }
                            await asyncio.sleep(0.005)  # æçŸ­å»¶è¿Ÿ

    
    # ä¿å­˜å¯¹è¯å¹¶æå–è®°å¿†
    if final_state and final_state.get("final_answer"):
        try:
            saved_memories = await save_conversation_and_extract_memories(
                session=session,
                session_id=session_id,
                user_query=user_query,
                assistant_reply=final_state["final_answer"],
                settings=settings,
                user_id=user_id,
                metadata={"thread_id": thread_id},
            )
            if saved_memories:
                logger.info(f"ğŸ’¾ æµå¼æ¨¡å¼ä¿å­˜äº† {len(saved_memories)} æ¡æ–°è®°å¿†")
        except Exception as e:
            logger.warning(f"æµå¼æ¨¡å¼ä¿å­˜è®°å¿†å¤±è´¥: {e}")
    
    # æµå¼ç»“æŸ
    yield {
        "event": "completed",
        "thread_id": thread_id,
        "timestamp": datetime.now().isoformat()
    }

